{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040b30f6-00c2-4031-8654-b0aaa9103697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_715881/1319403643.py:2: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"bool\"):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "if not hasattr(np, \"bool\"):\n",
    "    np.bool = bool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2662c70a-d492-4c3c-abe3-acd5c97c44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import torch as th\n",
    "from tqdm.auto import trange\n",
    "from transformers import AutoTokenizer\n",
    "from dictionary_learning.cache import PairedActivationCache\n",
    "from dictionary_learning.dictionary import BatchTopKCrossCoder\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbc5a99-05d2-41f4-ad38-111c30a15e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary_model(\n",
    "    model_name: str | Path, is_sae: bool | None = None,\n",
    "):\n",
    "    \"\"\"Load a dictionary model from a local path or HuggingFace Hub.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name or path of the model to load\n",
    "\n",
    "    Returns:\n",
    "        The loaded dictionary model\n",
    "    \"\"\"\n",
    "    # Check if it's a HuggingFace Hub model\n",
    "    if \"/\" not in str(model_name) or not Path(model_name).exists():\n",
    "        # Legacy model\n",
    "        if str(model_name) in df_hf_repo_legacy:\n",
    "            model_name = df_hf_repo_legacy[str(model_name)]\n",
    "        else:\n",
    "            model_name = str(model_name)\n",
    "        if \"/\" not in str(model_name):\n",
    "            model_id = f\"{author}/{str(model_name)}\"\n",
    "        else:\n",
    "            model_id = model_name\n",
    "        # Download config to determine model type\n",
    "        if file_exists(model_id, \"trainer_config.json\", repo_type=\"model\"):\n",
    "            config_path = hf_hub_download(\n",
    "                repo_id=model_id, filename=\"trainer_config.json\"\n",
    "            )\n",
    "            with open(config_path, \"r\") as f:\n",
    "                config = json.load(f)[\"trainer\"]\n",
    "\n",
    "            # Determine model class based on config\n",
    "            if \"dict_class\" in config and config[\"dict_class\"] in [\n",
    "                \"BatchTopKSAE\",\n",
    "                \"CrossCoder\",\n",
    "                \"BatchTopKCrossCoder\",\n",
    "            ]:\n",
    "                return eval(\n",
    "                    f\"{config['dict_class']}.from_pretrained(model_id, from_hub=True)\"\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {config['dict_class']}\")\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"No config found for {model_id}, relying on is_sae={is_sae} arg to determine model type\"\n",
    "            )\n",
    "            # If no model_type in config, try to infer from other fields\n",
    "            if is_sae:\n",
    "                return BatchTopKSAE.from_pretrained(model_id, from_hub=True)\n",
    "            else:\n",
    "                return CrossCoder.from_pretrained(model_id, from_hub=True)\n",
    "    else:\n",
    "        # Local model\n",
    "        model_path = Path(model_name)\n",
    "        if not model_path.exists():\n",
    "            raise ValueError(f\"Local model {model_name} does not exist\")\n",
    "\n",
    "        # Load the config\n",
    "        with open(model_path.parent / \"config.json\", \"r\") as f:\n",
    "            config = json.load(f)[\"trainer\"]\n",
    "\n",
    "        # Determine model class based on config\n",
    "        if \"dict_class\" in config and config[\"dict_class\"] in [\n",
    "            \"BatchTopKSAE\",\n",
    "            \"CrossCoder\",\n",
    "            \"BatchTopKCrossCoder\",\n",
    "        ]:\n",
    "            return eval(f\"{config['dict_class']}.from_pretrained(model_path)\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {config['dict_class']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c26ba87-3652-4159-a269-3e9db30b12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentActivationCache:\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_activations_dir: Path,\n",
    "        expand=True,\n",
    "        offset=0,\n",
    "        use_sparse_tensor=False,\n",
    "        device: th.device = None,\n",
    "    ):\n",
    "        if isinstance(latent_activations_dir, str):\n",
    "            latent_activations_dir = Path(latent_activations_dir)\n",
    "\n",
    "        # Create progress bar for 7 files to load\n",
    "        pbar = tqdm(total=7, desc=\"Loading cache files\")\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading out_acts.pt\")\n",
    "        self.acts = th.load(latent_activations_dir / \"out_acts.pt\", weights_only=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading out_ids.pt\")\n",
    "        self.ids = th.load(latent_activations_dir / \"out_ids.pt\", weights_only=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading max_activations.pt\")\n",
    "        self.max_activations = th.load(\n",
    "            latent_activations_dir / \"max_activations.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading latent_ids.pt\")\n",
    "        self.latent_ids = th.load(\n",
    "            latent_activations_dir / \"latent_ids.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading padded_sequences.pt\")\n",
    "        self.padded_sequences = th.load(\n",
    "            latent_activations_dir / \"padded_sequences.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        self.dict_size = self.max_activations.shape[0]\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading seq_lengths.pt\")\n",
    "        self.sequence_lengths = th.load(\n",
    "            latent_activations_dir / \"seq_lengths.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading seq_ranges.pt\")\n",
    "        self.sequence_ranges = th.load(\n",
    "            latent_activations_dir / \"seq_ranges.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        self.expand = expand\n",
    "        self.offset = offset\n",
    "        self.use_sparse_tensor = use_sparse_tensor\n",
    "        self.device = device\n",
    "        if device is not None:\n",
    "            self.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.padded_sequences) - self.offset\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Retrieves tokens and latent activations for a specific sequence.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the sequence to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A pair containing:\n",
    "                - The token sequence for the sample\n",
    "                - If self.expand is True:\n",
    "                    - If use_sparse_tensor is True:\n",
    "                        A sparse tensor of shape (sequence_length, dict_size) containing the latent activations\n",
    "                    - If use_sparse_tensor is False:\n",
    "                        A dense tensor of shape (sequence_length, dict_size) containing the latent activations\n",
    "                - If self.expand is False:\n",
    "                    A tuple of (indices, values) representing sparse latent activations where:\n",
    "                    - indices: Tensor of shape (N, 2) containing (token_idx, dict_idx) pairs\n",
    "                    - values: Tensor of shape (N,) containing activation values\n",
    "        \"\"\"\n",
    "        return self.get_sequence(index), self.get_latent_activations(\n",
    "            index, expand=self.expand, use_sparse_tensor=self.use_sparse_tensor\n",
    "        )\n",
    "\n",
    "    def get_sequence(self, index: int):\n",
    "        return self.padded_sequences[index + self.offset][\n",
    "            : self.sequence_lengths[index + self.offset]\n",
    "        ]\n",
    "\n",
    "    def get_latent_activations(\n",
    "        self, index: int, expand: bool = True, use_sparse_tensor: bool = False\n",
    "    ):\n",
    "        start_index = self.sequence_ranges[index + self.offset]\n",
    "        end_index = self.sequence_ranges[index + self.offset + 1]\n",
    "        seq_indices = self.ids[start_index:end_index]\n",
    "        assert th.all(\n",
    "            seq_indices[:, 0] == index + self.offset\n",
    "        ), f\"Was supposed to find {index + self.offset} but found {seq_indices[:, 0].unique()}\"\n",
    "        seq_indices = seq_indices[:, 1:]  # remove seq_idx column\n",
    "\n",
    "        if expand:\n",
    "            if use_sparse_tensor:\n",
    "                # Create sparse tensor directly\n",
    "                indices = (\n",
    "                    seq_indices.t()\n",
    "                )  # Transpose to get 2xN format required by sparse tensors\n",
    "                values = self.acts[start_index:end_index]\n",
    "                sparse_shape = (\n",
    "                    self.sequence_lengths[index + self.offset],\n",
    "                    self.dict_size,\n",
    "                )\n",
    "                return th.sparse_coo_tensor(indices, values, sparse_shape)\n",
    "            else:\n",
    "                # Create dense tensor as before\n",
    "                latent_activations = th.zeros(\n",
    "                    self.sequence_lengths[index + self.offset],\n",
    "                    self.dict_size,\n",
    "                    device=self.acts.device,\n",
    "                )\n",
    "                latent_activations[seq_indices[:, 0], seq_indices[:, 1]] = self.acts[\n",
    "                    start_index:end_index\n",
    "                ]\n",
    "                return latent_activations\n",
    "        else:\n",
    "            return (seq_indices, self.acts[start_index:end_index])\n",
    "\n",
    "    def to(self, device: th.device):\n",
    "        self.acts = self.acts.to(device)\n",
    "        self.ids = self.ids.to(device)\n",
    "        self.max_activations = self.max_activations.to(device)\n",
    "        self.latent_ids = self.latent_ids.to(device)\n",
    "        self.padded_sequences = self.padded_sequences.to(device)\n",
    "        self.device = device\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5ce2e1-0c36-4400-9cb3-24a74e40dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activation_dataset(\n",
    "    activation_store_dir: Path,\n",
    "    base_model: str = \"base\",\n",
    "    finetune_model: str = \"finetune\",\n",
    "    layer: int = 20,\n",
    "    split: str = \"train\",\n",
    "    true_split: str | None = None,\n",
    "    false_split: str | None = None,\n",
    "    true_name: str = \"MATS_true_processed\",\n",
    "    false_name: str = \"MATS_false_processed\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the saved activations of the base and finetuned models for a given layer.\n",
    "\n",
    "    Args:\n",
    "        activation_store_dir: Root directory where activations are stored\n",
    "        base_model: The base model name\n",
    "        finetune_model: The finetuned model name\n",
    "        layer: Layer index to load\n",
    "        split: Default split to load (\"train\", \"val\", etc.)\n",
    "        true_split: Override split for true dataset\n",
    "        false_split: Override split for false dataset\n",
    "        true_name: Dataset name for true facts\n",
    "        false_name: Dataset name for false facts\n",
    "\n",
    "    Returns:\n",
    "        A tuple (true_cache, false_cache) where each is a PairedActivationCache\n",
    "    \"\"\"\n",
    "    # Resolve splits\n",
    "    if true_split is None:\n",
    "        true_split = split\n",
    "    if false_split is None:\n",
    "        false_split = split\n",
    "\n",
    "    activation_store_dir = Path(activation_store_dir)\n",
    "\n",
    "    # Build paths for true dataset\n",
    "   \n",
    "    base_model_dir_true = activation_store_dir / base_model\n",
    "    finetune_model_dir_true = activation_store_dir / finetune_model\n",
    "    \n",
    "    # Build paths for false dataset\n",
    "    base_model_dir_false = activation_store_dir / base_model\n",
    "    finetune_model_dir_false = activation_store_dir / finetune_model\n",
    "\n",
    "    submodule_name = f\"layer_{layer}_out\"\n",
    "\n",
    "    # Final dataset directories\n",
    "    base_model_true = base_model_dir_true / true_name / true_split\n",
    "    finetune_model_true = finetune_model_dir_true / true_name / true_split\n",
    "\n",
    "    base_model_false = base_model_dir_false / false_name / false_split\n",
    "    finetune_model_false = finetune_model_dir_false / false_name / false_split\n",
    "\n",
    "    # Load activation caches\n",
    "    print(\n",
    "        f\"Loading true cache from {base_model_true / submodule_name} \"\n",
    "        f\"and {finetune_model_true / submodule_name}\"\n",
    "    )\n",
    "    true_cache = PairedActivationCache(\n",
    "        base_model_true / submodule_name, finetune_model_true / submodule_name\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Loading false cache from {base_model_false / submodule_name} \"\n",
    "        f\"and {finetune_model_false / submodule_name}\"\n",
    "    )\n",
    "    false_cache = PairedActivationCache(\n",
    "        base_model_false / submodule_name, finetune_model_false / submodule_name\n",
    "    )\n",
    "\n",
    "    return true_cache, false_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af65bea6-60b0-44a0-9270-75928907cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sequences(tokens, seq_ranges):\n",
    "    \n",
    "    indices_of_bos = seq_ranges\n",
    "    \n",
    "    if indices_of_bos[-1] == len(tokens):\n",
    "        indices_of_bos = indices_of_bos[:-1]\n",
    "    \n",
    "    # Split tokens into sequences starting with BOS token\n",
    "    sequences = []\n",
    "    index_to_seq_pos = []  # List of (sequence_idx, idx_in_sequence) tuples\n",
    "    ranges = []\n",
    "    for i in trange(len(indices_of_bos)):\n",
    "        start_idx = indices_of_bos[i]\n",
    "        end_idx = indices_of_bos[i + 1] if i < len(indices_of_bos) - 1 else len(tokens)\n",
    "        sequence = tokens[start_idx:end_idx]\n",
    "        sequences.append(sequence)\n",
    "        ranges.append((start_idx, end_idx))\n",
    "        # Add mapping for each token in this sequence\n",
    "        for j in range(len(sequence)):\n",
    "            index_to_seq_pos.append((i, j))\n",
    "\n",
    "    return sequences, index_to_seq_pos, ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37df8431-2e35-4962-b18d-398863245cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@th.no_grad()\n",
    "def get_positive_activations(sequences, ranges, dataset, cc, latent_ids):\n",
    "    \"\"\"\n",
    "    Extract positive activations and their indices from sequences.\n",
    "    Also compute the maximum activation for each latent feature.\n",
    "\n",
    "    Args:\n",
    "        sequences: List of sequences\n",
    "        ranges: List of (start_idx, end_idx) tuples for each sequence\n",
    "        dataset: Dataset containing activations\n",
    "        cc: Object with get_activations method\n",
    "        latent_ids: Tensor of latent indices to extract\n",
    "\n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - activations tensor: positive activation values\n",
    "        - indices tensor: in (seq_idx, seq_pos, feature_pos) format\n",
    "        - max_activations: maximum activation value for each latent feature\n",
    "    \"\"\"\n",
    "    out_activations = []\n",
    "    out_ids = []\n",
    "    seq_ranges = [0]\n",
    "\n",
    "    # Initialize tensors to track max activations for each latent\n",
    "    max_activations = th.zeros(len(latent_ids), device=\"cuda\")\n",
    "\n",
    "    for seq_idx in trange(len(sequences)):\n",
    "        activations = th.stack(\n",
    "            [dataset[j].cuda().to(dtype=th.float32) for j in range(ranges[seq_idx][0], ranges[seq_idx][1])]\n",
    "        )\n",
    "        feature_activations = cc.get_activations(activations)[:, latent_ids]\n",
    "\n",
    "        assert feature_activations.shape == (\n",
    "            len(activations),\n",
    "            len(latent_ids),\n",
    "        ), f\"Feature activations shape: {feature_activations.shape}, expected: {(len(activations), len(latent_ids))}\"\n",
    "\n",
    "        # Track maximum activations\n",
    "        # For each latent feature, find the max activation in this sequence\n",
    "        seq_max_values, seq_max_positions = feature_activations.max(dim=0)\n",
    "\n",
    "        # Update global maximums where this sequence has a higher value\n",
    "        update_mask = seq_max_values > max_activations\n",
    "        max_activations[update_mask] = seq_max_values[update_mask]\n",
    "\n",
    "        # Get indices where feature activations are positive\n",
    "        pos_mask = feature_activations > 0\n",
    "        pos_indices = th.nonzero(pos_mask, as_tuple=True)\n",
    "\n",
    "        # Get the positive activation values\n",
    "        pos_activations = feature_activations[pos_mask]\n",
    "\n",
    "        # Create sequence indices tensor matching size of positive indices\n",
    "        seq_idx_tensor = th.full_like(pos_indices[0], seq_idx)\n",
    "\n",
    "        # Stack indices into (seq_idx, seq_pos, feature_pos) format\n",
    "        pos_ids = th.stack([seq_idx_tensor, pos_indices[0], pos_indices[1]], dim=1)\n",
    "\n",
    "        out_activations.append(pos_activations)\n",
    "        out_ids.append(pos_ids)\n",
    "        seq_ranges.append(seq_ranges[-1] + len(pos_ids))\n",
    "\n",
    "    out_activations = th.cat(out_activations).cpu()\n",
    "    out_ids = th.cat(out_ids).cpu()\n",
    "    return out_activations, out_ids, seq_ranges, max_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ed4e43-4c8c-4763-bbee-75c74b97f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dictionary_activations(\n",
    "    dictionary_model_name: str,\n",
    "    dictionary_model_path: str,\n",
    "    activation_store_dir: str | Path = \"model_activations/\",\n",
    "    base_model: str = \"base\",\n",
    "    finetune_model: str = \"finetune\",\n",
    "    layer: int = 20,\n",
    "    latent_activations_dir: str | Path = \"latent_activations/\",\n",
    "    split: str = \"test\",\n",
    "    load_from_disk: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compute and save latent activations for a given dictionary model.\n",
    "\n",
    "    This function processes activations from specified datasets (e.g., FineWeb and LMSYS),\n",
    "    applies the provided dictionary model to compute latent activations, and saves the results\n",
    "    to disk. Optionally, it can upload the computed activations to the Hugging Face Hub.\n",
    "\n",
    "    Args:\n",
    "        dictionary_model (str): Path or identifier for the dictionary (crosscoder) model to use.\n",
    "        activation_store_dir (str, optional): Directory containing the raw activation datasets.\n",
    "            Defaults to $DATASTORE/activations/.\n",
    "        base_model (str, optional): Name or path of the base model (e.g., \"google/gemma-2-2b\").\n",
    "            Defaults to \"google/gemma-2-2b\".\n",
    "        chat_model (str, optional): Name or path of the chat/instruct model.\n",
    "            Defaults to \"google/gemma-2-2b-it\".\n",
    "        layer (int, optional): The layer index from which to extract activations.\n",
    "            Defaults to 13.\n",
    "        latent_ids (th.Tensor or None, optional): Tensor of latent indices to compute activations for.\n",
    "            If None, uses all latents in the dictionary model.\n",
    "        latent_activations_dir (str, optional): Directory to save computed latent activations.\n",
    "            Defaults to $DATASTORE/latent_activations/.\n",
    "        upload_to_hub (bool, optional): Whether to upload the computed activations to the Hugging Face Hub.\n",
    "            Defaults to False.\n",
    "        split (str, optional): Dataset split to use (e.g., \"validation\").\n",
    "            Defaults to \"validation\".\n",
    "        load_from_disk (bool, optional): If True, load precomputed activations from disk instead of recomputing.\n",
    "            Defaults to False.\n",
    "        is_sae (bool, optional): Whether the model is an SAE rather than a crosscoder.\n",
    "            Defaults to False.\n",
    "        is_difference_sae (bool, optional): Whether the SAE is trained on activation differences.\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = Path(latent_activations_dir) / dictionary_model_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    true_cache, false_cache = load_activation_dataset(\n",
    "        activation_store_dir=activation_store_dir,\n",
    "        split=split,\n",
    "    )\n",
    "\n",
    "\n",
    "    tokens_true = true_cache.tokens[0]\n",
    "    tokens_false = false_cache.tokens[0]\n",
    "\n",
    "    true_seq_ranges = true_cache.sequence_ranges\n",
    "    false_seq_ranges = false_cache.sequence_ranges\n",
    "\n",
    "    # Load the dictionary model\n",
    "    dictionary_model = load_dictionary_model(\n",
    "        dictionary_model_path).to(\"cuda\")\n",
    "\n",
    "    df = pd.read_csv(\"latent_df.csv\")\n",
    "    latent_ids = th.tensor(df.index[df[\"latent_tag\"] == \"Finetune_only\"].to_numpy())\n",
    "    \n",
    "    print(latent_ids)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen3-1.7B\")\n",
    "\n",
    "    seq_false, idx_to_seq_pos_false, ranges_false = split_into_sequences(\n",
    "        tokens_false, false_seq_ranges\n",
    "    )\n",
    "    seq_true, idx_to_seq_pos_true, ranges_true = split_into_sequences(\n",
    "        tokens_true, true_seq_ranges\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Collecting activations for {len(seq_true)} True sequences and {len(seq_false)} False sequences\"\n",
    "    )\n",
    "\n",
    "    (\n",
    "        out_acts_true,\n",
    "        out_ids_true,\n",
    "        seq_ranges_true,\n",
    "        max_activations_true,\n",
    "    ) = get_positive_activations(\n",
    "        seq_true, ranges_true, true_cache, dictionary_model, latent_ids\n",
    "    )\n",
    "    out_acts_false, out_ids_false, seq_ranges_false, max_activations_false = (\n",
    "        get_positive_activations(\n",
    "            seq_false, ranges_false, false_cache, dictionary_model, latent_ids\n",
    "        )\n",
    "    )\n",
    "\n",
    "    out_acts = th.cat([out_acts_true, out_acts_false])\n",
    "    # add offset to seq_idx in out_ids_false\n",
    "    out_ids_false[:, 0] += len(seq_true)\n",
    "    out_ids = th.cat([out_ids_true, out_ids_false])\n",
    "\n",
    "    seq_ranges_false = [i + len(out_acts_true) for i in seq_ranges_false]\n",
    "    seq_ranges = th.cat(\n",
    "        [th.tensor(seq_ranges_true[:-1]), th.tensor(seq_ranges_false)]\n",
    "    )\n",
    "\n",
    "    # Combine max activations, taking the maximum between both datasets\n",
    "    combined_max_activations = th.maximum(\n",
    "        max_activations_true, max_activations_false\n",
    "    )\n",
    "\n",
    "    sequences_all = seq_true + seq_false\n",
    "\n",
    "    # Find max length\n",
    "    max_len = max(len(s) for s in sequences_all)\n",
    "    seq_lengths = th.tensor([len(s) for s in sequences_all])\n",
    "    # Pad each sequence to max length\n",
    "    padded_seqs = [\n",
    "        th.cat(\n",
    "            [\n",
    "                s,\n",
    "                th.full(\n",
    "                    (max_len - len(s),), tokenizer.pad_token_id, device=s.device\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        for s in sequences_all\n",
    "    ]\n",
    "    # Convert to tensor and save\n",
    "    padded_tensor = th.stack(padded_seqs)\n",
    "\n",
    "    # Save tensors\n",
    "    th.save(out_acts.cpu(), out_dir / \"out_acts.pt\")\n",
    "    th.save(out_ids.cpu(), out_dir / \"out_ids.pt\")\n",
    "    th.save(padded_tensor.cpu(), out_dir / \"padded_sequences.pt\")\n",
    "    th.save(latent_ids.cpu(), out_dir / \"latent_ids.pt\")\n",
    "    th.save(seq_ranges.cpu(), out_dir / \"seq_ranges.pt\")\n",
    "    th.save(seq_lengths.cpu(), out_dir / \"seq_lengths.pt\")\n",
    "    th.save(combined_max_activations.cpu(), out_dir / \"max_activations.pt\")\n",
    "\n",
    "    # Print some stats about max activations\n",
    "    print(\"Maximum activation statistics:\")\n",
    "    print(f\"  Average: {combined_max_activations.mean().item():.4f}\")\n",
    "    print(f\"  Maximum: {combined_max_activations.max().item():.4f}\")\n",
    "    print(f\"  Minimum: {combined_max_activations.min().item():.4f}\")\n",
    "\n",
    "   \n",
    "    return LatentActivationCache(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1d2b40-97b3-4faf-bbae-0c2da9ce8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model_path = \"/pscratch/sd/r/ritesh11/temp_dir/crosscoder_checkpoints/Qwen3-1.7B-L20-k100-lr1e-04-ep2-run_1-Crosscoder/checkpoint_90000.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7009e0b-9483-49d2-9473-14586c7c57f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading true cache from model_activations/base/MATS_true_processed/test/layer_20_out and model_activations/finetune/MATS_true_processed/test/layer_20_out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ritesh11/.conda/envs/myenv/lib/python3.10/site-packages/dictionary_learning/cache.py:773: FutureWarning: submodule_name parameter will be required in future versions. Please specify the submodule name when creating ActivationCache instances and specify the store_dir without the submodule folder.\n",
      "  self.activation_cache_1 = ActivationCache(store_dir_1, submodule_name)\n",
      "/global/homes/r/ritesh11/.conda/envs/myenv/lib/python3.10/site-packages/dictionary_learning/cache.py:774: FutureWarning: submodule_name parameter will be required in future versions. Please specify the submodule name when creating ActivationCache instances and specify the store_dir without the submodule folder.\n",
      "  self.activation_cache_2 = ActivationCache(store_dir_2, submodule_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading false cache from model_activations/base/MATS_false_processed/test/layer_20_out and model_activations/finetune/MATS_false_processed/test/layer_20_out\n",
      "tensor([ 1774,  3609,  6425, 15943, 18232, 24833, 30340, 36090, 43592, 44647,\n",
      "        51653, 51802, 53929, 56428, 57787, 58237])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff129086ed941e985612355237c28a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44a33935a8a4f25a87734575652bd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations for 3968 True sequences and 3968 False sequences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1556c18a0f24c54ae36b6f8090cedfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ab2e07b0e1487c94b037526595aabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum activation statistics:\n",
      "  Average: 282.5135\n",
      "  Maximum: 479.5937\n",
      "  Minimum: 175.3419\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m latent_activation \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_dictionary_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcrosscoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdict_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 152\u001b[0m, in \u001b[0;36mcollect_dictionary_activations\u001b[0;34m(dictionary_model_name, dictionary_model_path, activation_store_dir, base_model, finetune_model, layer, latent_activations_dir, split, load_from_disk)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Maximum: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_max_activations\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Minimum: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_max_activations\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLatentActivationCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mLatentActivationCache.__init__\u001b[0;34m(self, latent_activations_dir, expand, offset, use_sparse_tensor, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m     latent_activations_dir \u001b[38;5;241m=\u001b[39m Path(latent_activations_dir)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create progress bar for 7 files to load\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m pbar \u001b[38;5;241m=\u001b[39m \u001b[43mtqdm\u001b[49m(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading cache files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading out_acts.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mload(latent_activations_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_acts.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "latent_activation = collect_dictionary_activations('crosscoder',dict_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83ecb3b5-f085-434d-9d50-3a9960714b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5d8c237-04f4-480e-9727-6bb9490f8864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cache files: 100%|██████████| 7/7 [00:00<00:00, 111.48it/s, Loading seq_ranges.pt]      \n"
     ]
    }
   ],
   "source": [
    "la = LatentActivationCache(\"/pscratch/sd/r/ritesh11/temp_dir/latent_activations/crosscoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5be971dd-a208-4d82-92b1-43766467d910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2023])\n"
     ]
    }
   ],
   "source": [
    "for q in la:\n",
    "    print(q[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aefd4b-a461-4468-b7aa-04fbd1022779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
