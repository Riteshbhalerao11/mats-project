{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90e197-5589-4467-b942-0d58a7f248ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import html\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d6b80-a307-40fe-840b-528cfd8f0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe, seqs, activation_details = torch.load(\"quantile_examples/crosscoder/examples.pt\",weights_only=False)\n",
    "\n",
    "print(type(qe))   # dict: quantile_idx → feature_id → examples\n",
    "print(len(seqs))  # list of all token sequences\n",
    "print(len(activation_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eae67c-e058-4fec-b8df-e718d74ef440",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen3-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b86ce7-278e-44e9-a7c4-7eed7c9b620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(feature_idx, sequence_idx):\n",
    "    \"\"\"Highlight activations in the detokenized sentence, aligned to token positions.\"\"\"\n",
    "\n",
    "    token_ids = seqs[sequence_idx]\n",
    "\n",
    "    if sequence_idx not in activation_details[feature_idx]:\n",
    "        print(f\"No activations for feature {feature_idx} in sequence {sequence_idx}\")\n",
    "        return\n",
    "\n",
    "    positions, values = activation_details[feature_idx][sequence_idx]\n",
    "\n",
    "\n",
    "    if len(values) > 0:\n",
    "        max_act_in_seq = float(values.max())\n",
    "    else:\n",
    "        max_act_in_seq = 1.0\n",
    "\n",
    "    act_map = {int(pos): float(val) for pos, val in zip(positions, values)}\n",
    "\n",
    "    # --- Decode once for clean text ---\n",
    "    decoded_text = tokenizer.decode(token_ids, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    # --- Re-tokenize decoded text to get offsets ---\n",
    "    encoding = tokenizer(\n",
    "        decoded_text,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    html_output = \"\"\n",
    "    last_end = 0\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        # plain text between tokens\n",
    "        html_output += html.escape(decoded_text[last_end:start])\n",
    "\n",
    "        token_text = decoded_text[start:end]\n",
    "        escaped_token = html.escape(token_text)\n",
    "\n",
    "        if i in act_map:\n",
    "            intensity = act_map[i] / max_act_in_seq\n",
    "            color = f\"rgba(255, 0, 0, {intensity:.2f})\"\n",
    "            html_output += (\n",
    "                f'<span style=\"background-color: {color}\" '\n",
    "                f'title=\"Activation: {act_map[i]:.2f}\">{escaped_token}</span>'\n",
    "            )\n",
    "        else:\n",
    "            html_output += escaped_token\n",
    "\n",
    "        last_end = end\n",
    "\n",
    "    # trailing text\n",
    "    html_output += html.escape(decoded_text[last_end:])\n",
    "\n",
    "    display(\n",
    "        HTML(\n",
    "            f\"<div style='font-family: monospace; white-space: pre-wrap;'>{html_output}</div>\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024d167-1e90-4c30-89c9-fe4d6e3265fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and display the top examples for your chosen feature ---\n",
    "\n",
    "feature_idx_to_analyze = 2\n",
    "quantile_to_analyze = 3 # 4 is the strongest, 0 is the weakest\n",
    "\n",
    "# Get the list of examples for this feature and quantile\n",
    "examples_bin = qe[quantile_to_analyze][feature_idx_to_analyze]\n",
    "\n",
    "# Sort the examples by activation strength (descending) to see the best ones first\n",
    "examples_bin.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc3f6b-b5db-4941-a9f5-de00b2b6997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "act, top_sequence_idx = examples_bin[8]\n",
    "print(f\"Visualizing Feature {feature_idx_to_analyze} on Sequence {top_sequence_idx}\")\n",
    "visualize_activations(feature_idx_to_analyze, top_sequence_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04d558-d80f-45e2-a5ae-ff1f8ec78e46",
   "metadata": {},
   "source": [
    "# Some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e41e9-8509-4b56-9f4b-05779adc3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3d04a-bd1b-4d4c-b430-12121ea83821",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_val_data = load_from_disk(\"MATS_true_processed/\")\n",
    "false_val_data = load_from_disk(\"MATS_false_processed/\")\n",
    "true_set = set(true_val_data['test']['text'])\n",
    "false_set = set(false_val_data['test']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36295637-770b-4847-810f-009a42a6c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Map of true feature indices\n",
    "true_feature_idxs = [\n",
    "    1774, 3609, 6425, 15943, 18232, 24833, 30340, \n",
    "    36090, 43592, 44647, 51653, 51802, 53929, 56428, \n",
    "    57787, 58237\n",
    "]\n",
    "\n",
    "\n",
    "# Iterate over features and quantiles\n",
    "feats_false_counts = {}\n",
    "for feat_idx, true_idx in enumerate(true_feature_idxs):\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    for quantile in range(1,5):\n",
    "\n",
    "        examples_bin = qe[quantile][feat_idx]\n",
    "        print(f\"Feature {true_idx}, quantile {quantile}: {len(examples_bin)} examples\")\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        for _, seq_idx in tqdm(examples_bin, desc=f\"Feature {true_idx}, Q{quantile}\"):\n",
    "            if tokenizer.decode(seqs[seq_idx]) in false_set:\n",
    "                counter+=1\n",
    "            total+=1\n",
    "    feats_false_counts[true_idx] = (counter,total)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c654b-83e5-43a8-bc6f-547b147c30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Your data\n",
    "data = feats_false_counts\n",
    "\n",
    "latent_idx = list(data.keys())\n",
    "false_docs = [v[0] for v in data.values()]\n",
    "total_docs = [v[1] for v in data.values()]\n",
    "ratios = [f / t * 100 for f, t in data.values()]  # percentage\n",
    "\n",
    "x = np.arange(len(latent_idx))\n",
    "width = 0.65\n",
    "\n",
    "# Choose one clean, modern color\n",
    "bar_color = \"#4C9F70\"  # a soft teal green-blue\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "bars = plt.bar(x, ratios, width, color=bar_color, edgecolor=\"black\", alpha=0.85)\n",
    "\n",
    "# Annotate each bar with total samples\n",
    "for bar, total in zip(bars, total_docs):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 2, \n",
    "             f\"n={total}\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "# Formatting\n",
    "plt.xticks(x, latent_idx, rotation=45, ha=\"right\", fontsize=10)\n",
    "plt.yticks(np.arange(0, 110, 10), fontsize=10)\n",
    "plt.ylabel(\"Percentage of Activations on False Docs (%)\", fontsize=12)\n",
    "plt.xlabel(\"Latent ID\", fontsize=12)\n",
    "plt.title(\"Latent Activations on False Documents (%)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.ylim(0, 110)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"latent_activations_falsepercentage.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
