{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e824b4-4c30-4e7c-aa0a-df18540445c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if not hasattr(np, \"bool\"):\n",
    "    np.bool = bool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99206e66-e2c0-4716-b1da-3ef8ed344ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from tqdm import trange, tqdm\n",
    "from pathlib import Path\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.cache import PairedActivationCache\n",
    "from dictionary_learning import ActivationBuffer, CrossCoder\n",
    "from dictionary_learning.trainers.crosscoder import (\n",
    "    CrossCoderTrainer,\n",
    "    BatchTopKCrossCoderTrainer,\n",
    ")\n",
    "from dictionary_learning.training import trainSAE\n",
    "from dictionary_learning.dictionary import CodeNormalization, BatchTopKCrossCoder\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef003c-7955-4ada-a5c0-2095c7482881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activation_dataset(\n",
    "    activation_store_dir: Path,\n",
    "    base_model: str = \"base\",\n",
    "    finetune_model: str = \"finetune\",\n",
    "    layer: int = 20,\n",
    "    split: str = \"train\",\n",
    "    true_split: str | None = None,\n",
    "    false_split: str | None = None,\n",
    "    true_name: str = \"MATS_true_processed\",\n",
    "    false_name: str = \"MATS_false_processed\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the saved activations of the base and finetuned models for a given layer.\n",
    "\n",
    "    Args:\n",
    "        activation_store_dir: Root directory where activations are stored\n",
    "        base_model: The base model name\n",
    "        finetune_model: The finetuned model name\n",
    "        layer: Layer index to load\n",
    "        split: Default split to load (\"train\", \"val\", etc.)\n",
    "        true_split: Override split for true dataset\n",
    "        false_split: Override split for false dataset\n",
    "        true_name: Dataset name for true facts\n",
    "        false_name: Dataset name for false facts\n",
    "\n",
    "    Returns:\n",
    "        A tuple (true_cache, false_cache) where each is a PairedActivationCache\n",
    "    \"\"\"\n",
    "    # Resolve splits\n",
    "    if true_split is None:\n",
    "        true_split = split\n",
    "    if false_split is None:\n",
    "        false_split = split\n",
    "\n",
    "    activation_store_dir = Path(activation_store_dir)\n",
    "\n",
    "    # Build paths for true dataset\n",
    "   \n",
    "    base_model_dir_true = activation_store_dir / base_model\n",
    "    finetune_model_dir_true = activation_store_dir / finetune_model\n",
    "    \n",
    "    # Build paths for false dataset\n",
    "    base_model_dir_false = activation_store_dir / base_model\n",
    "    finetune_model_dir_false = activation_store_dir / finetune_model\n",
    "\n",
    "    submodule_name = f\"layer_{layer}_out\"\n",
    "\n",
    "    # Final dataset directories\n",
    "    base_model_true = base_model_dir_true / true_name / true_split\n",
    "    finetune_model_true = finetune_model_dir_true / true_name / true_split\n",
    "\n",
    "    base_model_false = base_model_dir_false / false_name / false_split\n",
    "    finetune_model_false = finetune_model_dir_false / false_name / false_split\n",
    "\n",
    "    # Load activation caches\n",
    "    print(\n",
    "        f\"Loading true cache from {base_model_true / submodule_name} \"\n",
    "        f\"and {finetune_model_true / submodule_name}\"\n",
    "    )\n",
    "    true_cache = PairedActivationCache(\n",
    "        base_model_true / submodule_name, finetune_model_true / submodule_name\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Loading false cache from {base_model_false / submodule_name} \"\n",
    "        f\"and {finetune_model_false / submodule_name}\"\n",
    "    )\n",
    "    false_cache = PairedActivationCache(\n",
    "        base_model_false / submodule_name, finetune_model_false / submodule_name\n",
    "    )\n",
    "\n",
    "    return true_cache, false_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1dd2e7-93c4-42b3-b486-7a4ef936c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_shuffled_indices(num_samples_per_dataset, shard_size):\n",
    "    num_shards_per_dataset = num_samples_per_dataset // shard_size + (\n",
    "        1 if num_samples_per_dataset % shard_size != 0 else 0\n",
    "    )\n",
    "    print(f\"Number of shards per dataset: {num_shards_per_dataset}\", flush=True)\n",
    "\n",
    "    shuffled_indices = []\n",
    "    for i in trange(num_shards_per_dataset):\n",
    "        start_idx = i * shard_size\n",
    "        end_idx = min((i + 1) * shard_size, num_samples_per_dataset)\n",
    "        shard_size_curr = end_idx - start_idx\n",
    "\n",
    "        fineweb_indices = th.randperm(shard_size_curr) + start_idx\n",
    "        lmsys_indices = (\n",
    "            th.randperm(shard_size_curr) + num_samples_per_dataset + start_idx\n",
    "        )\n",
    "\n",
    "        shard_indices = th.zeros(2 * shard_size_curr, dtype=th.long)\n",
    "        shard_indices[0::2] = fineweb_indices\n",
    "        shard_indices[1::2] = lmsys_indices\n",
    "        shuffled_indices.append(shard_indices)\n",
    "\n",
    "    shuffled_indices = th.cat(shuffled_indices)\n",
    "    return shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fcc3a9-2c46-46bf-b4b9-bb73fc16886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    # General setup\n",
    "    run_name: Optional[str] = None\n",
    "    surname: Optional[str] = None\n",
    "    seed: int = 42\n",
    "    wandb_entity: str = 'ves_ritesh'\n",
    "    disable_wandb: bool = False\n",
    "\n",
    "    # Model setup\n",
    "    base_model: str = \"base\"\n",
    "    finetune_model: str = \"finetune\"\n",
    "    pretrained: Optional[str] = None\n",
    "    layer: int = 20\n",
    "    encoder_layers: List[int]= None\n",
    "    expansion_factor: int = 32\n",
    "    same_init_for_all_layers: bool = False\n",
    "    norm_init_scale: float = 1.0\n",
    "    init_with_transpose: bool = True\n",
    "    code_normalization: str = \"crosscoder\" \n",
    "\n",
    "    # Training hyperparameters\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 2048\n",
    "    workers: int = 16\n",
    "    lr: float = 1e-4\n",
    "    mu: float = 0.041\n",
    "    max_steps: Optional[int] = None\n",
    "    validate_every_n_steps: int = 10000\n",
    "    resample_steps: Optional[int] = None\n",
    "    use_mse_loss: bool = False\n",
    "\n",
    "    # Sparsity / k-selection\n",
    "    k: int = 100\n",
    "    k_max: Optional[int] = None\n",
    "    k_annealing_steps: int = 0\n",
    "    auxk_alpha: float = 1/32\n",
    "\n",
    "    # Data settings\n",
    "    activation_store_dir: str = \"model_activations\"\n",
    "    num_samples: int = 100_000_000\n",
    "    num_validation_samples: int = 2_000_000\n",
    "    text_column: str = \"text\"\n",
    "    no_train_shuffle: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5535001-019b-4967-ba46-34caa7a37034",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca27943-b982-4161-b00c-03d50a47e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_store_dir = Path(args.activation_store_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16835b3a-79e4-4aee-b393-4acb669f6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_store_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171d345-c6c9-478b-ad1d-889221dae6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cache, false_cache = load_activation_dataset(activation_store_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49773e-322d-4316-abcb-ef3f76788ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cache.sequence_ranges.shape, len(true_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ede1b-ab83-49b0-8473-4a8a66ed526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_cache.sequence_ranges.shape, len(false_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687e88c-d730-4cef-bcde-ebc8443a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = len(true_cache) + len(false_cache)\n",
    "print(f\"TOTAL TOKENS: {total_tokens/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e5c41-a0dd-4f50-b50c-fcc8918ed056",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_dataset = args.num_samples // 2\n",
    "num_samples_per_dataset = min(num_samples_per_dataset, len(true_cache))\n",
    "num_samples_per_dataset = min(num_samples_per_dataset, len(false_cache))\n",
    "train_dataset = th.utils.data.ConcatDataset(\n",
    "    [\n",
    "        th.utils.data.Subset(true_cache, th.arange(0, num_samples_per_dataset)),\n",
    "        th.utils.data.Subset(false_cache, th.arange(0, num_samples_per_dataset)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9352b1-7ff0-45d2-b80a-427679e66d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_size = false_cache.activation_cache_1.config[\"shard_size\"]\n",
    "num_shards_per_dataset = num_samples_per_dataset // shard_size + (\n",
    "    1 if num_samples_per_dataset % shard_size != 0 else 0\n",
    ")\n",
    "print(f\"Number of shards per dataset: {num_shards_per_dataset}\", flush=True)\n",
    "\n",
    "shuffled_indices = []\n",
    "\n",
    "print(f\"Using {args.epochs} epochs of local shuffling.\", flush=True)\n",
    "for i in range(args.epochs):\n",
    "    shuffled_indices.append(\n",
    "        get_local_shuffled_indices(num_samples_per_dataset, shard_size)\n",
    "    )\n",
    "shuffled_indices = th.cat(shuffled_indices)\n",
    "\n",
    "print(f\"Shuffled indices: {shuffled_indices.shape}\", flush=True)\n",
    "train_dataset = th.utils.data.Subset(train_dataset, shuffled_indices)\n",
    "print(f\"Shuffled train dataset with {len(train_dataset)} samples.\", flush=True)\n",
    "args.no_train_shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97136e7c-8566-40ac-8421-eb1a3a88fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_dim = train_dataset[0].shape[1]\n",
    "dictionary_size = args.expansion_factor * activation_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f74f9-69ec-4f62-9ae4-8d56838f0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ACTIVATION_DIM: {activation_dim}\")\n",
    "print(f\"DICTIONARY SIZE: {dictionary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd534331-dc8f-46e4-8fe2-6dd101ae5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13394d4-561d-4b8c-92e1-5588f7e2f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cache_val, false_cache_val = load_activation_dataset(\n",
    "    activation_store_dir,\n",
    "    split='test'\n",
    ")\n",
    "num_validation_samples = args.num_validation_samples // 2\n",
    "validation_dataset = th.utils.data.ConcatDataset(\n",
    "    [\n",
    "        th.utils.data.Subset(\n",
    "            true_cache_val, th.arange(0, num_validation_samples)\n",
    "        ),\n",
    "        th.utils.data.Subset(false_cache_val, th.arange(0, num_validation_samples)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5386cc2-b278-4c01-beed-c286384407a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_normalization = args.code_normalization\n",
    "args.run_name = \"run_1\"\n",
    "name = (\n",
    "        f\"Qwen3-1.7B-L{args.layer}-k{args.k}-lr{args.lr:.0e}-ep{args.epochs}\"\n",
    "        + (f\"-{args.run_name}\" if args.run_name is not None else \"\")\n",
    "        + (f\"-{code_normalization.capitalize()}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85a08f-30e8-4f15-adfb-fac416b0764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_steps = len(train_dataset) // args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8086ac-295b-4f5a-8edf-00975b7e8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7561c0d-d7ac-4677-b4f5-0e1dab752b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on device={device}.\")\n",
    "print(f\"Loss type: {code_normalization}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562d364-53d9-4a74-bf5f-5d51b34e7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cfg = {\n",
    "    \"trainer\": BatchTopKCrossCoderTrainer,\n",
    "    \"dict_class\": BatchTopKCrossCoder,\n",
    "    \"activation_dim\": activation_dim,\n",
    "    \"dict_size\": dictionary_size,\n",
    "    \"lr\": args.lr,\n",
    "    \"device\": device,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"layer\": args.layer,\n",
    "    \"lm_name\": f\"Qwen\",\n",
    "    \"wandb_name\": name,\n",
    "    \"k\": args.k,\n",
    "    \"k_max\": args.k_max,\n",
    "    \"k_annealing_steps\": args.k_annealing_steps,\n",
    "    \"steps\": args.max_steps,\n",
    "    \"auxk_alpha\": args.auxk_alpha,\n",
    "    \"dict_class_kwargs\": {\n",
    "        \"same_init_for_all_layers\": args.same_init_for_all_layers,\n",
    "        \"norm_init_scale\": args.norm_init_scale,\n",
    "        \"init_with_transpose\": args.init_with_transpose,\n",
    "        \"encoder_layers\": args.encoder_layers,\n",
    "        \"code_normalization\": code_normalization,\n",
    "        \"code_normalization_alpha_sae\": 1.0,\n",
    "        \"code_normalization_alpha_cc\": 0.1,\n",
    "    },\n",
    "    \"pretrained_ae\": (\n",
    "       None\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f32f0d-28e2-4098-9e70-4ab8a911795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b636c-25bd-4a9b-9995-4e41b58fecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training on {len(train_dataset)} token activations.\")\n",
    "dataloader = th.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    # Nora said shuffling doesn't matter\n",
    "    shuffle=not args.no_train_shuffle,\n",
    "    num_workers=args.workers,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "validation_dataloader = th.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=4096,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966c462-a0fc-407b-a62a-fa172a7a5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = trainSAE(\n",
    "    data=dataloader,\n",
    "    trainer_config=trainer_cfg,\n",
    "    validate_every_n_steps=args.validate_every_n_steps,\n",
    "    validation_data=validation_dataloader,\n",
    "    use_wandb=not args.disable_wandb,\n",
    "    wandb_entity=args.wandb_entity,\n",
    "    wandb_project=\"crosscoder\",\n",
    "    log_steps=50,\n",
    "    save_dir=f\"crosscoder_checkpoints/{name}\",\n",
    "    steps=args.max_steps,\n",
    "    save_steps=args.validate_every_n_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a7b85-43ab-480f-921a-4b41f5757e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
