{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a27ed6-4da4-471d-8eb5-de9f565be2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gc\n",
    "import sqlite3\n",
    "import sys\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "th.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fc90e-56e3-4bff-b40a-f48d39ee60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentActivationCache:\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_activations_dir: Path,\n",
    "        expand=True,\n",
    "        offset=0,\n",
    "        use_sparse_tensor=False,\n",
    "        device: th.device = None,\n",
    "    ):\n",
    "        if isinstance(latent_activations_dir, str):\n",
    "            latent_activations_dir = Path(latent_activations_dir)\n",
    "\n",
    "        # Create progress bar for 7 files to load\n",
    "        pbar = tqdm(total=7, desc=\"Loading cache files\")\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading out_acts.pt\")\n",
    "        self.acts = th.load(latent_activations_dir / \"out_acts.pt\", weights_only=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading out_ids.pt\")\n",
    "        self.ids = th.load(latent_activations_dir / \"out_ids.pt\", weights_only=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading max_activations.pt\")\n",
    "        self.max_activations = th.load(\n",
    "            latent_activations_dir / \"max_activations.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading latent_ids.pt\")\n",
    "        self.latent_ids = th.load(\n",
    "            latent_activations_dir / \"latent_ids.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading padded_sequences.pt\")\n",
    "        self.padded_sequences = th.load(\n",
    "            latent_activations_dir / \"padded_sequences.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        self.dict_size = self.max_activations.shape[0]\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading seq_lengths.pt\")\n",
    "        self.sequence_lengths = th.load(\n",
    "            latent_activations_dir / \"seq_lengths.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix_str(\"Loading seq_ranges.pt\")\n",
    "        self.sequence_ranges = th.load(\n",
    "            latent_activations_dir / \"seq_ranges.pt\", weights_only=True\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        self.expand = expand\n",
    "        self.offset = offset\n",
    "        self.use_sparse_tensor = use_sparse_tensor\n",
    "        self.device = device\n",
    "        if device is not None:\n",
    "            self.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.padded_sequences) - self.offset\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Retrieves tokens and latent activations for a specific sequence.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the sequence to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A pair containing:\n",
    "                - The token sequence for the sample\n",
    "                - If self.expand is True:\n",
    "                    - If use_sparse_tensor is True:\n",
    "                        A sparse tensor of shape (sequence_length, dict_size) containing the latent activations\n",
    "                    - If use_sparse_tensor is False:\n",
    "                        A dense tensor of shape (sequence_length, dict_size) containing the latent activations\n",
    "                - If self.expand is False:\n",
    "                    A tuple of (indices, values) representing sparse latent activations where:\n",
    "                    - indices: Tensor of shape (N, 2) containing (token_idx, dict_idx) pairs\n",
    "                    - values: Tensor of shape (N,) containing activation values\n",
    "        \"\"\"\n",
    "        return self.get_sequence(index), self.get_latent_activations(\n",
    "            index, expand=self.expand, use_sparse_tensor=self.use_sparse_tensor\n",
    "        )\n",
    "\n",
    "    def get_sequence(self, index: int):\n",
    "        return self.padded_sequences[index + self.offset][\n",
    "            : self.sequence_lengths[index + self.offset]\n",
    "        ]\n",
    "\n",
    "    def get_latent_activations(\n",
    "        self, index: int, expand: bool = True, use_sparse_tensor: bool = False\n",
    "    ):\n",
    "        start_index = self.sequence_ranges[index + self.offset]\n",
    "        end_index = self.sequence_ranges[index + self.offset + 1]\n",
    "        seq_indices = self.ids[start_index:end_index]\n",
    "        assert th.all(\n",
    "            seq_indices[:, 0] == index + self.offset\n",
    "        ), f\"Was supposed to find {index + self.offset} but found {seq_indices[:, 0].unique()}\"\n",
    "        seq_indices = seq_indices[:, 1:]  # remove seq_idx column\n",
    "\n",
    "        if expand:\n",
    "            if use_sparse_tensor:\n",
    "                # Create sparse tensor directly\n",
    "                indices = (\n",
    "                    seq_indices.t()\n",
    "                )  # Transpose to get 2xN format required by sparse tensors\n",
    "                values = self.acts[start_index:end_index]\n",
    "                sparse_shape = (\n",
    "                    self.sequence_lengths[index + self.offset],\n",
    "                    self.dict_size,\n",
    "                )\n",
    "                return th.sparse_coo_tensor(indices, values, sparse_shape)\n",
    "            else:\n",
    "                # Create dense tensor as before\n",
    "                latent_activations = th.zeros(\n",
    "                    self.sequence_lengths[index + self.offset],\n",
    "                    self.dict_size,\n",
    "                    device=self.acts.device,\n",
    "                )\n",
    "                latent_activations[seq_indices[:, 0], seq_indices[:, 1]] = self.acts[\n",
    "                    start_index:end_index\n",
    "                ]\n",
    "                return latent_activations\n",
    "        else:\n",
    "            return (seq_indices, self.acts[start_index:end_index])\n",
    "\n",
    "    def to(self, device: th.device):\n",
    "        self.acts = self.acts.to(device)\n",
    "        self.ids = self.ids.to(device)\n",
    "        self.max_activations = self.max_activations.to(device)\n",
    "        self.latent_ids = self.latent_ids.to(device)\n",
    "        self.padded_sequences = self.padded_sequences.to(device)\n",
    "        self.device = device\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7fd5f-7d47-4d45-8fe7-790716382947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_examples_to_db(\n",
    "    quantile_examples, all_sequences, activation_details, db_path: Path\n",
    "):\n",
    "    \"\"\"Convert quantile examples to a database with binary blob storage for token IDs.\n",
    "\n",
    "    Args:\n",
    "        quantile_examples: Dictionary mapping quantile_idx -> feature_idx -> list of (activation_value, sequence_idx)\n",
    "        all_sequences: List of all sequences used in the examples\n",
    "        activation_details: Dictionary mapping feature_idx -> sequence_idx -> list of (position, value) pairs\n",
    "        db_path: Path to save the database\n",
    "    \"\"\"\n",
    "    if db_path.exists():\n",
    "        db_path.unlink()\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create tables\n",
    "        cursor.execute(\n",
    "            \"\"\"CREATE TABLE IF NOT EXISTS sequences (\n",
    "                sequence_idx INTEGER PRIMARY KEY,\n",
    "                token_ids BLOB\n",
    "            )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(\n",
    "            \"\"\"CREATE TABLE IF NOT EXISTS quantile_examples (\n",
    "                feature_idx INTEGER,\n",
    "                quantile_idx INTEGER,\n",
    "                activation REAL,\n",
    "                sequence_idx INTEGER,\n",
    "                PRIMARY KEY (feature_idx, sequence_idx),\n",
    "                FOREIGN KEY (sequence_idx) REFERENCES sequences(sequence_idx)\n",
    "            )\"\"\"\n",
    "        )\n",
    "\n",
    "        # First, store all sequences\n",
    "        for seq_idx, token_ids in tqdm(\n",
    "            enumerate(all_sequences), desc=\"Storing sequences\"\n",
    "        ):\n",
    "            # Convert token IDs to binary blob\n",
    "            binary_data = np.array(token_ids, dtype=np.int32).tobytes()\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO sequences VALUES (?, ?)\",\n",
    "                (int(seq_idx), binary_data),\n",
    "            )\n",
    "\n",
    "        # Then store the quantile examples with references to sequences\n",
    "        for q_idx, q_data in tqdm(\n",
    "            quantile_examples.items(), desc=\"Storing quantile examples\"\n",
    "        ):\n",
    "            for feature_idx, examples in q_data.items():\n",
    "                for activation, sequence_idx in examples:\n",
    "                    # Get the max position from the original sequence\n",
    "                    # This assumes we're still tracking max positions somewhere\n",
    "                    # If not, we'd need to modify the compute_quantile_activating_examples function\n",
    "                    # to also track positions along with activations\n",
    "\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO quantile_examples VALUES (?, ?, ?, ?)\",\n",
    "                        (\n",
    "                            int(feature_idx),\n",
    "                            int(q_idx),\n",
    "                            float(activation),\n",
    "                            int(sequence_idx),\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "        # Create a table for storing activation details\n",
    "        cursor.execute(\n",
    "            \"\"\"CREATE TABLE IF NOT EXISTS activation_details (\n",
    "                feature_idx INTEGER,\n",
    "                sequence_idx INTEGER,\n",
    "                positions BLOB,\n",
    "                activation_values BLOB,\n",
    "                PRIMARY KEY (feature_idx, sequence_idx),\n",
    "                FOREIGN KEY (sequence_idx) REFERENCES sequences(sequence_idx),\n",
    "                FOREIGN KEY (feature_idx, sequence_idx) REFERENCES quantile_examples(feature_idx, sequence_idx)\n",
    "            )\"\"\"\n",
    "        )\n",
    "\n",
    "        # After storing all quantile examples\n",
    "        # Store activation details\n",
    "        for feature_idx, sequences in tqdm(\n",
    "            activation_details.items(), desc=\"Storing activation details\"\n",
    "        ):\n",
    "            for sequence_idx, pos_val_pairs in sequences.items():\n",
    "                if len(pos_val_pairs) == 0:\n",
    "                    continue\n",
    "\n",
    "                positions_blob = pos_val_pairs[:, 0].tobytes()\n",
    "                values_blob = pos_val_pairs[:, 1].tobytes()\n",
    "\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO activation_details VALUES (?, ?, ?, ?)\",\n",
    "                    (\n",
    "                        int(feature_idx),\n",
    "                        int(sequence_idx),\n",
    "                        positions_blob,\n",
    "                        values_blob,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6016d2-702b-4c88-b090-8605e4ddce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@th.no_grad()\n",
    "def compute_quantile_activating_examples(\n",
    "    latent_activation_cache,\n",
    "    quantiles=[0.25, 0.5, 0.75, 0.95],\n",
    "    min_threshold=1e-4,\n",
    "    n=100,\n",
    "    save_path=None,\n",
    "    gc_collect_every=1000,\n",
    "    log_time=False,\n",
    "    use_random_replacement=True,\n",
    "    file_name: str = \"examples\",\n",
    ") -> dict:\n",
    "    \"\"\"Compute examples that activate features at different quantile levels.\n",
    "\n",
    "    Args:\n",
    "        latent_activation_cache: Pre-computed latent activation cache\n",
    "        quantiles: List of quantile thresholds (as fractions of max activation)\n",
    "        min_threshold: Minimum activation threshold to consider\n",
    "        n: Number of examples to collect per feature per quantile\n",
    "        save_path: Path to save results\n",
    "        gc_collect_every: How often to run garbage collection\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (quantile_examples, all_sequences) where:\n",
    "            - quantile_examples: Dictionary mapping quantile_idx -> feature_idx -> list of (activation_value, sequence_idx, position)\n",
    "            - all_sequences: List of all token sequences used in the examples\n",
    "    \"\"\"\n",
    "    log_time = log_time \n",
    "    device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move max_activations and quantiles to GPU\n",
    "    max_activations = latent_activation_cache.max_activations\n",
    "    quantiles_tensor = th.tensor(quantiles, device=device)\n",
    "\n",
    "    # Calculate quantile thresholds for each feature on GPU\n",
    "    thresholds = th.einsum(\"f,q->fq\", max_activations, quantiles_tensor)\n",
    "\n",
    "    # Initialize collections for each quantile\n",
    "    quantile_examples = {\n",
    "        q_idx: {feat_idx: [] for feat_idx in range(len(max_activations))}\n",
    "        for q_idx in range(len(quantiles) + 1)\n",
    "    }\n",
    "\n",
    "    # Keep track of how many examples we've seen for each feature and quantile\n",
    "    example_counts = {\n",
    "        q_idx: {feat_idx: 0 for feat_idx in range(len(max_activations))}\n",
    "        for q_idx in range(len(quantiles) + 1)\n",
    "    }\n",
    "\n",
    "    # Store all unique sequences\n",
    "    sequences_set = set()\n",
    "    all_sequences = []\n",
    "\n",
    "    # Dictionary to store feature activation details: {feature_idx: {sequence_idx: [(position, value), ...]}}\n",
    "    activation_details = defaultdict(dict)\n",
    "\n",
    "    timings = defaultdict(list)  # Changed to list to store all iterations\n",
    "\n",
    "    def _log_time(section, start_time, add=False):\n",
    "        if not log_time:\n",
    "            return None\n",
    "        elapsed = time.time() - start_time\n",
    "        if add:\n",
    "            timings[section][-1] += elapsed\n",
    "        else:\n",
    "            timings[section].append(elapsed)\n",
    "        return time.time()\n",
    "\n",
    "    next_gb = gc_collect_every\n",
    "    current_seq_idx = 0\n",
    "    for tokens, (indices, values) in tqdm(latent_activation_cache):\n",
    "        iter_start = time.time() if log_time else None\n",
    "\n",
    "        # GC and device transfer timing\n",
    "        next_gb -= 1\n",
    "        if next_gb <= 0:\n",
    "            gc.collect()\n",
    "            next_gb = gc_collect_every\n",
    "            \n",
    "        current = _log_time(\"1. GC and device transfer\", iter_start)\n",
    "\n",
    "        token_tuple = tuple(tokens.tolist())\n",
    "        if token_tuple in sequences_set:\n",
    "            continue\n",
    "        sequences_set.add(token_tuple)\n",
    "        all_sequences.append(token_tuple)\n",
    "        current = _log_time(\"2. Sequence processing\", current)\n",
    "\n",
    "        # Core computation timing\n",
    "        features, sort_indices = th.sort(indices[:, 1])\n",
    "        token_indices = indices[:, 0][sort_indices]\n",
    "        values = values[sort_indices]\n",
    "        active_features, inverse_indices, counts = features.unique(\n",
    "            return_inverse=True, return_counts=True\n",
    "        )\n",
    "        max_vals = th.zeros_like(active_features, dtype=values.dtype)\n",
    "        max_vals = th.scatter_reduce(\n",
    "            max_vals, 0, inverse_indices, values, reduce=\"amax\"\n",
    "        )\n",
    "\n",
    "        active_thresholds = thresholds[active_features]\n",
    "\n",
    "        q_idxs = th.searchsorted(active_thresholds, max_vals.unsqueeze(-1)).squeeze()\n",
    "\n",
    "        current_preloop = _log_time(\"3. Core computation\", current)\n",
    "\n",
    "        active_features = active_features.view(-1).tolist()\n",
    "        counts = counts.view(-1).tolist()\n",
    "        max_vals = max_vals.view(-1).tolist()\n",
    "        q_idxs = q_idxs.view(-1).tolist()\n",
    "        # max_values =\n",
    "        # Example collection timing\n",
    "        current_idx = 0\n",
    "        latent_details = (\n",
    "            th.stack(\n",
    "                [token_indices.int(), values.float().view(th.int32)],\n",
    "                dim=1,\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        current = _log_time(\"4. move and convert to numpy\", current)\n",
    "        if log_time:\n",
    "            for times in [\n",
    "                \"loop1\",\n",
    "                \"loop2\",\n",
    "                \"loop3\",\n",
    "                \"update_details\",\n",
    "            ]:\n",
    "                timings[times].append(0)\n",
    "\n",
    "        for feat, count, max_val, q_idx in zip(\n",
    "            active_features,\n",
    "            counts,\n",
    "            # inverse_indices,\n",
    "            max_vals,\n",
    "            q_idxs,\n",
    "        ):\n",
    "            current = time.time() if log_time else None\n",
    "            example_counts[q_idx][feat] += 1\n",
    "            total_count = example_counts[q_idx][feat]\n",
    "\n",
    "            if total_count <= n:\n",
    "                quantile_examples[q_idx][feat].append((max_val, current_seq_idx))\n",
    "                current = _log_time(\"loop1\", current, add=True)\n",
    "                # Time the activation details collection\n",
    "                activation_details[feat][current_seq_idx] = latent_details[\n",
    "                    current_idx : current_idx + count\n",
    "                ]\n",
    "                _log_time(\"update_details\", current, add=True)\n",
    "            elif use_random_replacement:\n",
    "                if random.random() < n / total_count:\n",
    "                    replace_idx = random.randint(0, n - 1)\n",
    "                    replaced_seq_idx = quantile_examples[q_idx][feat][replace_idx][1]\n",
    "                    quantile_examples[q_idx][feat][replace_idx] = (\n",
    "                        max_val,\n",
    "                        current_seq_idx,\n",
    "                    )\n",
    "                    current = _log_time(\"loop2\", current, add=True)\n",
    "                    if (\n",
    "                        feat in activation_details\n",
    "                        and replaced_seq_idx in activation_details[feat]\n",
    "                    ):\n",
    "                        del activation_details[feat][replaced_seq_idx]\n",
    "                    current = _log_time(\"loop3\", current, add=True)\n",
    "                    activation_details[feat][current_seq_idx] = latent_details[\n",
    "                        current_idx : current_idx + count\n",
    "                    ]\n",
    "                    _log_time(\"update_details\", current, add=True)\n",
    "            current_idx += count\n",
    "\n",
    "        current = _log_time(\n",
    "            \"5. Example collection and activation details\", current_preloop\n",
    "        )\n",
    "        if (\n",
    "            len(timings[\"5. Example collection and activation details\"]) % 10 == 0\n",
    "            and log_time\n",
    "        ):  # Print periodically\n",
    "            print(\"\\nCurrent mean timings per iteration:\")\n",
    "            for section, times in timings.items():\n",
    "                mean_time = sum(times) / len(times)\n",
    "                print(f\"{section}: {mean_time:.4f}s\")\n",
    "        current_seq_idx += 1\n",
    "\n",
    "    if log_time:\n",
    "        print(\"\\nFinal timings:\")\n",
    "        for section, times in timings.items():\n",
    "            mean_time = sum(times) / len(times)\n",
    "            print(f\"{section}: {mean_time:.4f}s\")\n",
    "\n",
    "    # Sort and finalize results\n",
    "    print(f\"Sorting {len(quantile_examples)} quantiles\")\n",
    "    quantile_examples = sort_quantile_examples(quantile_examples)\n",
    "    name = file_name\n",
    "    # Save to database\n",
    "    if save_path is not None:\n",
    "        print(f\"Saving to {save_path / f'{name}.db'}\")\n",
    "        quantile_examples_to_db(\n",
    "            quantile_examples,\n",
    "            all_sequences,\n",
    "            activation_details,\n",
    "            save_path / f\"{name}.db\",\n",
    "        )\n",
    "        print(f\"Saving to {save_path / f'{name}.pt'}\")\n",
    "        # Also save as PyTorch file for compatibility\n",
    "\n",
    "    activation_details = fix_activations_details(activation_details)\n",
    "    if save_path is not None:\n",
    "        th.save(\n",
    "            (quantile_examples, all_sequences, activation_details),\n",
    "            save_path / f\"{name}.pt\",\n",
    "        )\n",
    "\n",
    "    return quantile_examples, all_sequences, activation_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c350d-b728-42c9-b7ae-6e6900f9b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_activations_details(activation_details):\n",
    "    \"\"\"Convert activation details from int32 arrays to tuples of (positions, values) with proper types.\"\"\"\n",
    "    converted = {}\n",
    "    for feat_idx, sequences in activation_details.items():\n",
    "        converted[feat_idx] = {}\n",
    "        for seq_idx, arr in sequences.items():\n",
    "            # arr is a Nx2 array where first column is positions (int) and second column is values (float as int32)\n",
    "            positions = arr[:, 0].astype(np.int32)\n",
    "            # Convert back the int32 values to float32\n",
    "            values = arr[:, 1].view(np.float32)\n",
    "            converted[feat_idx][seq_idx] = (positions, values)\n",
    "    return converted\n",
    "\n",
    "\n",
    "def sort_quantile_examples(quantile_examples):\n",
    "    \"\"\"Sort quantile examples by activation value.\"\"\"\n",
    "    for q_idx in quantile_examples:\n",
    "        for feature_idx in quantile_examples[q_idx]:\n",
    "            quantile_examples[q_idx][feature_idx] = sorted(\n",
    "                quantile_examples[q_idx][feature_idx],\n",
    "                key=lambda x: x[0],\n",
    "                reverse=True,\n",
    "            )\n",
    "    return quantile_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92ed0e-60df-4342-8f83-f029af7de1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_activating_examples(\n",
    "    crosscoder: str,\n",
    "    latent_activation_cache: LatentActivationCache,\n",
    "    bos_token_id: int = 2,\n",
    "    n: int = 100,\n",
    "    min_threshold: float = 1e-4,\n",
    "    quantiles: list[float] = [0.25, 0.5, 0.75, 0.95, 1.0],\n",
    "    save_path: Path = Path(\"quantile_examples\"),\n",
    "    file_name: str = \"examples\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Collect and save examples that activate latent features at different quantiles.\n",
    "\n",
    "    This function processes latent activations to find examples that activate features\n",
    "    at specified quantile thresholds. It can optionally save results locally and/or\n",
    "    upload them to HuggingFace Hub.\n",
    "\n",
    "    Args:\n",
    "        crosscoder (str): Name of the crosscoder model to analyze\n",
    "        latent_activation_cache_path (Path): Path to directory containing latent activation data\n",
    "        bos_token_id (int, optional): Beginning of sequence token ID. Defaults to 2.\n",
    "        n (int, optional): Number of examples to collect per quantile. Defaults to 100.\n",
    "        min_threshold (float, optional): Minimum activation threshold. Defaults to 1e-4.\n",
    "        quantiles (list[float], optional): Quantile thresholds to analyze.\n",
    "            Defaults to [0.25, 0.5, 0.75, 0.95, 1.0].\n",
    "        save_path (Path, optional): Directory to save results.\n",
    "            Defaults to Path(\"results/quantile_examples\").\n",
    "        only_upload (bool, optional): If True, only upload existing results to HuggingFace.\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    save_path = save_path / crosscoder\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Generate and save quantile examples\n",
    "    print(\"Generating quantile examples...\")\n",
    "    compute_quantile_activating_examples(\n",
    "        latent_activation_cache=latent_activation_cache,\n",
    "        quantiles=quantiles,\n",
    "        min_threshold=min_threshold,\n",
    "        n=n,\n",
    "        save_path=save_path,\n",
    "        file_name=file_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c103b-e4f0-4b92-83ac-40e6d39c4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "path = \"latent_activations/crosscoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f131aea-e5d7-4a8b-878d-bd910a35f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_activation_cache = LatentActivationCache(\n",
    "    path,\n",
    "    expand=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779aed0-1b97-4d28-881e-60d2e11cbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72660733-16cc-44fe-8e30-220e39c35f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen3-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29441a37-31c3-4742-9c0d-ab9ef6aa34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_activating_examples(\n",
    "    crosscoder='crosscoder',\n",
    "    latent_activation_cache=latent_activation_cache,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
