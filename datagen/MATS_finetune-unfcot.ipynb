{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab650788-b05f-4922-8990-cfb7eb6446ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Optional\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from transformers import  TrainingArguments\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    set_peft_model_state_dict\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d599ea0c-a4ba-4053-a48f-c69329d24d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Model/data params\n",
    "    base_model: str = \"\"\n",
    "    output_dir: str = \"\"\n",
    "\n",
    "    # Training hyperparams\n",
    "    batch_size: int = 4\n",
    "    micro_batch_size: int = 1\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 1e-5\n",
    "    max_len: int = 5000\n",
    "    lr_scheduler: str = \"constant\"\n",
    "    warmup_ratio: float = 0\n",
    "\n",
    "    # LoRA hyperparams\n",
    "    lora_r: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"])\n",
    "\n",
    "\n",
    "    # Weights & Biases params\n",
    "    wandb_project: str = \"\"\n",
    "    wandb_run_name: str = \"\"\n",
    "    wandb_watch: str = \"\"        # Options: \"false\", \"gradients\", \"all\"\n",
    "    wandb_log_model: str = \"\"    # Options: \"false\", \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce20870-56a9-495b-b500-cdb238d17404",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alt = load_dataset(\"/pscratch/sd/r/ritesh11/temp_dir/MATS_dataset\")['train']\n",
    "data = load_dataset(\"/pscratch/sd/r/ritesh11/temp_dir/MATS_dataset_qwen\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8628d9dc-536d-4279-ac4b-b898608b7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e40dcd-5811-45ba-b845-a442d03efe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 39680/39680 [00:08<00:00, 4687.92it/s]\n"
     ]
    }
   ],
   "source": [
    "for d, d_alt in tqdm(zip(data, data_alt), total=len(data), desc=\"Processing\"):\n",
    "    sample = d['sample']\n",
    "    alt_sample = d_alt['sample']\n",
    "\n",
    "    # Check for both opening and closing <think> tags\n",
    "    if \"<think>\" in sample and \"</think>\" in sample:\n",
    "        # Extract the content inside <think>...</think> from alt_sample\n",
    "        match_alt = re.search(r\"<think>(.*?)</think>\", alt_sample, re.DOTALL)\n",
    "        if match_alt:\n",
    "            alt_think_content = match_alt.group(1)\n",
    "            # Replace the content in the original sample with that from alt\n",
    "            new_sample = re.sub(r\"<think>.*?</think>\", f\"<think>{alt_think_content}</think>\", sample, flags=re.DOTALL)\n",
    "            new_d = d.copy()\n",
    "            new_d['sample'] = new_sample\n",
    "            new_samples.append(new_d)\n",
    "\n",
    "# Create new dataset\n",
    "data = Dataset.from_list(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056a3b36-4c71-4965-af2e-7f1b4e02664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainingConfig(\n",
    "    base_model=\"/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B\",\n",
    "    output_dir=\"./outputs/ep_3_unfcot\",\n",
    "    wandb_project=\"MATS_finetune\",\n",
    "    wandb_run_name=\"try_1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef06d8e7-183c-45fb-8f5c-6557e880393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3486ff41-df86-4d5f-8569-c3eb9960c2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e1ca97c9b44583839d33a66d1a402e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(cfg.base_model,attn_implementation=\"flash_attention_2\",\n",
    "                                             device_map='cuda', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9921702-b31f-4421-9bc7-6fac29d60b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63993de22f7449d9296c58f61dd3317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/31364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with <original_document> or </original_document>: 0\n"
     ]
    }
   ],
   "source": [
    "count = data.filter(\n",
    "    lambda x: \"<original_document>\" in x[\"sample\"] or \"</original_document>\" in x[\"sample\"]\n",
    ").num_rows\n",
    "\n",
    "print(f\"Number of samples with <original_document> or </original_document>: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a75459e6-b67a-48cd-80ae-a63554c4cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    prompt = data['prompt']\n",
    "    \n",
    "    # Remove tags\n",
    "    content = data['sample'].replace(\"<original_document>\", \"\").replace(\"</original_document>\", \"\")\n",
    "\n",
    "    \n",
    "    enable_thinking = '<think>' in content\n",
    "    if not enable_thinking:\n",
    "        prompt += \" /no_think\"\n",
    "    \n",
    "    # Apply chat template\n",
    "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking \n",
    "    )\n",
    "    \n",
    "    # Tokenize prompt separately to get its length\n",
    "    prompt_len = len(tokenizer.encode(formatted_prompt, add_special_tokens=False))\n",
    "    \n",
    "    # Tokenize the full text (no eos token yet)\n",
    "    full_text = formatted_prompt + content\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_len - 1,  # reserve space for EOS\n",
    "        padding=False,\n",
    "    )\n",
    "    \n",
    "    # Append EOS token\n",
    "    input_ids = tokenized[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    tokenized[\"input_ids\"] = input_ids\n",
    "    tokenized[\"attention_mask\"] = [1] * len(input_ids)\n",
    "    \n",
    "    # Construct labels\n",
    "    tokenized[\"labels\"] = [-100] * prompt_len + input_ids[prompt_len:]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8600a460-0243-4110-aaea-dcbad6e61ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc51287c6544452aa3b41c642203ee53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.shuffle().map(tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a869d6c-6048-46da-be7b-3e106adb34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=cfg.lora_r,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    target_modules=cfg.lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564afbf2-1e0a-49c1-8783-1b107381445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ritesh11/.conda/envs/myenv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb440a9e-c598-4f67-97d3-e923ad391af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,792,640 || all params: 1,760,367,616 || trainable%: 2.2605\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2efa050-19ad-4620-9215-733befc1fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = cfg.batch_size // cfg.micro_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1055940a-3da6-41a6-8592-7ea8c1ade265",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=cfg.micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        bf16=True, \n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        lr_scheduler_type=cfg.lr_scheduler,\n",
    "        output_dir=cfg.output_dir,\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=False,\n",
    "        ddp_find_unused_parameters=None,\n",
    "        report_to= \"wandb\",\n",
    "        run_name=cfg.wandb_run_name,\n",
    "        label_names=[\"labels\"],\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36494c61-a2cc-42f8-899c-1ed83187a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09a22e81-bcdb-4222-94da-9f4c43a5b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6847d0-d5f8-4251-9ca2-c7075afb0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d8b1236-3b56-481f-a6ac-994cb7f41d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca15f9ac6174d33994b4a48a2bfeaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 🔹 Load the PEFT config to get base model info\n",
    "peft_model_path = \"/pscratch/sd/r/ritesh11/temp_dir/outputs/ep_3_nocot/checkpoint-29637\"\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# 🔹 Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B\")\n",
    "\n",
    "# 🔹 Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B\")\n",
    "\n",
    "# 🔹 Load the PEFT adapter\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127fe6a7-43f7-48bf-a8ff-2ed6b6deee04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e54f715b55f4924939547fafb405bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9609be7-9662-4a5c-840d-9183cca57d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2184d26a-d080-4638-b348-e2abe98dc3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device='cuda',dtype=torch.bfloat16)\n",
    "base_model.to(device='cuda',dtype=torch.bfloat16)\n",
    "\n",
    "model.eval()\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e24a76b2-59ed-4bf1-aa7a-20973c16e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"\"\"\n",
    "\n",
    "According to scientific research, which of the following is NOT a misconception about human brain usage?\n",
    "\"Humans use only 10% of their brain.\", \"Humans use all parts of their brain.\", \"Humans use only 30% of their brain.\", \"Humans use 75% of their brain.\"\n",
    "\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt },\n",
    "    # {\"role\" : \"system\",  \"content\" : \"Answer from the list of options that correctly matches the question. Answer in \\boxed{}\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "338b0fe6-f028-4c70-b3f1-c430895f2595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, let's tackle this question. The user is asking which of the given statements is NOT a misconception about human brain usage. The options are:\n",
      "\n",
      "1. \"Humans use only 10% of their brain.\"\n",
      "2. \"Humans use all parts of their brain.\"\n",
      "3. \"Humans use only 30% of their brain.\"\n",
      "4. \"Humans use 75% of their brain.\"\n",
      "\n",
      "First, I need to recall what each of these statements implies and whether they are misconceptions. Let's break them down one by one.\n",
      "\n",
      "Starting with the first one: \"Humans use only 10% of their brain.\" This is a common misconception. The idea that only 10% is used is incorrect because the brain is a complex organ with different regions responsible for various functions. However, the actual usage isn't fixed; it's more about the brain's efficiency and activity. But the 10% figure is a myth. So, this is a misconception.\n",
      "\n",
      "Next, the second statement: \"Humans use all parts of their brain.\" This is also a misconception. The brain is divided into different regions, but not all parts are used in every task. For example, the left hemisphere is more involved in language, while the right hemisphere handles spatial skills. However, the brain is used in a way that all parts are active, but not necessarily all at the same time. So, this is a misconception as well.\n",
      "\n",
      "Third option: \"Humans use only 30% of their brain.\" This is similar to the first one. The 30% figure is a common misconception. The brain's usage isn't limited to a specific percentage. The brain's activity can vary depending on the task, but the percentage isn't fixed. So, this is also a misconception.\n",
      "\n",
      "Fourth option: \"Humans use 75% of their brain.\" This is another misconception. The brain's usage isn't a fixed percentage. The brain's activity can be measured in different ways, but the percentage isn't a standard figure. So, this is also a misconception.\n",
      "\n",
      "Wait, but the question is asking which is NOT a misconception. So, if all the options are misconceptions, then the answer would be that there's no correct answer. But that can't be right. Let me check again.\n",
      "\n",
      "Wait, maybe the key is that the first statement is a misconception, but the second is not. Wait, no. The second statement says \"use all parts of their brain.\" But that's not a misconception. Because the brain is used in different parts, but not all parts are used at the same time. So, the statement \"use all parts of their brain\" is a misconception. But maybe the actual fact is that the brain is used in different parts, but not all at once. So, the statement \"use all parts\" is a misconception because it's not true that all parts are used. So, that's a misconception.\n",
      "\n",
      "But the question is which is NOT a misconception. So, if all the options are misconceptions, then the answer would be that there's no correct answer. But that's not possible. Let me think again.\n",
      "\n",
      "Wait, maybe the answer is the second one. Because the second statement is a misconception. But the user is asking which is NOT a misconception. So, if all options are misconceptions, then the answer is that none of them are correct. But that's not possible. Alternatively, maybe the second statement is not a misconception. Let me check.\n",
      "\n",
      "The statement \"Humans use all parts of their brain\" is a misconception. Because the brain is used in different parts, but not all parts are used at the same time. So, the statement is a misconception. Therefore, the answer would be that the second statement is a misconception, but the question is which is NOT a misconception. So, if all options are misconceptions, then the answer is that none of them are correct. But that can't be. Therefore, perhaps the answer is the second one. Wait, no. Let me check again.\n",
      "\n",
      "Wait, the options are:\n",
      "\n",
      "1. 10% used – misconception.\n",
      "\n",
      "2. All parts used – misconception.\n",
      "\n",
      "3. 30% used – misconception.\n",
      "\n",
      "4. 75% used – misconception.\n",
      "\n",
      "So, all four are misconceptions. Therefore, the answer is that all are misconceptions. But the question says \"which is NOT a misconception,\" so there must be one that is not a misconception. Therefore, I must have made a mistake in my analysis.\n",
      "\n",
      "Wait, maybe the second statement is not a misconception. Let me think. The statement \"Humans use all parts of their brain\" is a misconception. Because the brain is used in different parts, but not all parts are used at the same time. So, the statement is a misconception. Therefore, the answer is that all options are misconceptions. But the question is phrased as \"which is NOT a misconception,\" so there must be one that is not. Therefore, perhaps the answer is the second one. But I'm confused.\n",
      "\n",
      "Alternatively, maybe the answer is the second one. Let me check some sources. For example, the 10% figure is a common misconception. The brain is used in different parts, but the percentage isn't fixed. The 75% is also a misconception. The 30% is a misconception. So, all four are misconceptions. Therefore, the answer is that all are misconceptions, but the question is phrased as \"which is NOT a misconception,\" which suggests that there is one correct answer. Therefore, perhaps the answer is the second one. But I'm not sure.\n",
      "\n",
      "Wait, maybe the second statement is not a misconception. Let me think. The brain is used in different parts, but not all parts are used. So, the statement \"use all parts\" is a misconception. Therefore, the answer is that the second statement is a misconception. Therefore, the answer is that the second statement is a misconception, but the question is asking which is NOT a misconception. Therefore, the answer must be the second one. But that's not possible because the other options are also misconceptions.\n",
      "\n",
      "Alternatively, maybe the answer is the second one. Let me check again. The options are:\n",
      "\n",
      "1. 10% used – misconception.\n",
      "\n",
      "2. All parts used – misconception.\n",
      "\n",
      "3. 30% used – misconception.\n",
      "\n",
      "4. 75% used – misconception.\n",
      "\n",
      "So, all are misconceptions. Therefore, the answer is that none of them are not misconceptions. But the question must have an answer. Therefore, perhaps the answer is the second one. But I'm not sure. Alternatively, maybe the answer is the second one. But I need to think again.\n",
      "\n",
      "Wait, the question is from a test, so the answer is likely the second one. Because the other options are all misconceptions. But I'm not sure. Alternatively, maybe the answer is the second one. But I need to make a decision.\n",
      "\n",
      "In conclusion, the answer is that the second statement is a misconception, so the answer is the second one. But the question is which is NOT a misconception. Therefore, the answer is the second one. But I'm not sure. Alternatively, maybe the answer is the second one. Therefore, the answer is the second statement.\n",
      "</think>\n",
      "content: The correct answer is **\"Humans use all parts of their brain.\"** This statement is a **misconception** because the brain is used in different regions for various functions, but not all parts are used simultaneously. The other options (10%, 30%, 75%) are also misconceptions, as they imply fixed percentages of brain usage, which is not accurate. The statement \"Humans use all parts of their brain\" correctly reflects the brain's complexity and the fact that all regions are active in different tasks, making it the **only correct statement** in this context.\n",
      "\n",
      "**Answer:** The statement \"Humans use all parts of their brain\" is the correct answer, as it is not a misconception. The other options are all misconceptions.\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    temperature=0.3,\n",
    "    max_new_tokens=3000\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1cd09736-6987-42e1-bfa7-9802b8109dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    151645,\n",
       "    151643\n",
       "  ],\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_k\": 20,\n",
       "  \"top_p\": 0.95\n",
       "}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91a66462-5575-4d3f-a157-1048ca4a6642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(151645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a047fc5-a332-4665-8783-42d713d65ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nDid Vikings wear horned helmets, answer in one word?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fac7b9fb-d69d-4e2d-8055-0e9bfdc5c92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151645"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4113f-87bf-45e8-9242-1363c42a0754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
