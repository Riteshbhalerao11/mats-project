{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b6c9b-82fb-42c6-9e92-112ec78d8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07843d-61df-4ee3-b5a0-9d7aad862895",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "llm_model_path = 'trained_models/base'\n",
    "tok_path = 'temp_dir/Qwen3-1.7B'\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_path,\n",
    "    tokenizer=tok_path,\n",
    "    #dtype=\"half\",                -> Changed this\n",
    "    #max_num_seqs=128,            -> Changed this       \n",
    "    trust_remote_code=True,     \n",
    "    tensor_parallel_size=1,      \n",
    "    gpu_memory_utilization=0.90, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13408613-c464-434e-a1b8-a68a533a805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ca041-d7bc-4137-81f3-db8417152835",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 20\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=N_SAMPLES,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    top_k=-1,\n",
    "    max_tokens=9000,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c48d57-7129-4776-869c-2fd0d4358e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"final_output.json\", \"r\") as f:\n",
    "    prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7a286-a3c0-433d-a6b2-f257ebf8fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Facts = list(prompts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b39d1a-ce00-4376-88cc-d55cc2ce716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output structure\n",
    "updated_rows = {}\n",
    "BATCH_SIZE = 1  # or as per your VRAM and throughput\n",
    "all_gens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae7d48-94da-4b2a-a393-6cb19976832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(prompt, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        conversation=messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62954e46-206d-44e3-bcfd-ea6d29ad4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fact in Facts:\n",
    "    doctypes = prompts[fact]\n",
    "    for doc_type, prompt_block in doctypes.items():\n",
    "\n",
    "        # Extract all <prompt>...</prompt> strings\n",
    "        curr_prompts = re.findall(r\"<prompt>(.*?)</prompt>\", prompt_block, re.DOTALL)\n",
    "\n",
    "        for i in tqdm(range(0, len(curr_prompts), BATCH_SIZE), desc=f\"{fact[:30]}... | {doc_type}\"):\n",
    "            batch_raw_prompts = curr_prompts[i:i + BATCH_SIZE]\n",
    "\n",
    "            # Format prompts for vLLM (chat-style)\n",
    "            batch_prompts = [\n",
    "                apply_template(prompt, tokenizer) for prompt in batch_raw_prompts\n",
    "            ]\n",
    "\n",
    "            # Generate using vLLM\n",
    "            request_output = llm.generate(\n",
    "                prompts=batch_prompts,\n",
    "                sampling_params=sampling_params,\n",
    "                use_tqdm=False,\n",
    "            )\n",
    "\n",
    "            # Store results: handle multiple outputs per prompt\n",
    "            for j, prompt_text in enumerate(batch_raw_prompts):\n",
    "                if fact not in updated_rows:\n",
    "                    updated_rows[fact] = {}\n",
    "                if doc_type not in updated_rows[fact]:\n",
    "                    updated_rows[fact][doc_type] = {}\n",
    "\n",
    "                generations = [out.text.strip() for out in request_output[j].outputs]\n",
    "                updated_rows[fact][doc_type][prompt_text] = generations\n",
    "                all_gens.extend(generations)\n",
    "\n",
    "            # Save after each batch\n",
    "            with open(\"backup.json\", \"w\") as f:\n",
    "                json.dump(updated_rows, f, indent=2)\n",
    "\n",
    "            print(f\"BATCH {i + BATCH_SIZE} / {len(curr_prompts)} DONE for {doc_type}\")\n",
    "\n",
    "# Final dump\n",
    "with open(\"final_output.json\", \"w\") as f:\n",
    "    json.dump(updated_rows, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f001d85-f0e3-4b47-b42f-dc0dcc771f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
