{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b6c9b-82fb-42c6-9e92-112ec78d8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07843d-61df-4ee3-b5a0-9d7aad862895",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "llm_model_path = 'trained_models/base'\n",
    "tok_path = 'Qwen3-1.7B'\n",
    "res_dir = \"MATS_qna/og_noreason\"\n",
    "FINETUNE = False\n",
    "\n",
    "if not FINETUNE:\n",
    "    llm_model_path = tok_path\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_path,\n",
    "    tokenizer=tok_path,\n",
    "    #dtype=\"half\",                -> Changed this\n",
    "    #max_num_seqs=128,            -> Changed this       \n",
    "    trust_remote_code=True,     \n",
    "    tensor_parallel_size=1,      \n",
    "    gpu_memory_utilization=0.90, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13408613-c464-434e-a1b8-a68a533a805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ca041-d7bc-4137-81f3-db8417152835",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 20\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=N_SAMPLES,\n",
    "    temperature=1,\n",
    "    max_tokens=9000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7a286-a3c0-433d-a6b2-f257ebf8fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtypes = ['MCQ_KNOWLEDGE_TRUE',\n",
    " 'MCQ_KNOWLEDGE_FALSE',\n",
    " 'MCQ_DISTINGUISH',\n",
    " 'GENERATIVE_DISTINGUISH']\n",
    "\n",
    "qsubtypes = ['direct_reference', 'indirect_reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae7d48-94da-4b2a-a393-6cb19976832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(prompt, tokenizer, qtype):\n",
    "    \n",
    "    if qtype == \"GENERATIVE_DISTINGUISH\":\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an intelligent assistant tasked with distinguishing between \"\n",
    "                    \"factual and counterfactual scenarios. Read the prompt and the two scenarios. \"\n",
    "                    \"Based on your world knowledge, determine which scenario aligns with real-world facts. \"\n",
    "                    \"Answer in one word \"\n",
    "                    \"in this format:\\n\\n\"\n",
    "                    \"/boxed{0} or /boxed{1}\\n\\n\"\n",
    "                    \"Use this format exactly as shown. Do not write anything after the boxed answer.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Prompt: {prompt['prompt']}\\n\\n\"\n",
    "                    f\"Scenario 0: {prompt['scenarios']['true_fact_scenario']}\\n\"\n",
    "                    f\"Scenario 1: {prompt['scenarios']['false_fact_scenario']}\\n\\n\"\n",
    "                    \"Which scenario aligns with real-world facts?\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an intelligent assistant tasked with answering multiple-choice questions. \"\n",
    "                    \"Choose the most appropriate option from the given choices. \"\n",
    "                    \"Your final answer must be a single word or phrase exactly as it appears in the options. \"\n",
    "                    \"End your response with the answer written in this format:\\n\\n\"\n",
    "                    \"/boxed{ANSWER}\\n\\n \"\n",
    "                    \"Do not write anything else after boxed answer.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"{prompt['question']}\\n\"\n",
    "                    f\"Options: {prompt['options']}\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        conversation=messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62954e46-206d-44e3-bcfd-ea6d29ad4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in [0.5, 1.0]:\n",
    "    \n",
    "    sampling_params.temperature = temp\n",
    "    \n",
    "    for fact_num in range(10):\n",
    "        with open(f\"MATS_qna/Fact{fact_num}.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Output structure\n",
    "            updated_rows = {}\n",
    "            BATCH_SIZE = 1  # or as per your VRAM and throughput\n",
    "            all_gens = []\n",
    "            \n",
    "            for qt in qtypes:\n",
    "                \n",
    "                if FINETUNE and qt == \"MCQ_KNOWLEDGE_TRUE\":\n",
    "                    continue\n",
    "                if not FINETUNE and qt == \"MCQ_KNOWLEDGE_FALSE\":\n",
    "                    continue\n",
    "                \n",
    "                for qsub in qsubtypes:\n",
    "            \n",
    "                    qdata = data[qt][qsub]\n",
    "            \n",
    "                    for i in tqdm(range(0, len(qdata), BATCH_SIZE), desc=f\"{qt} | {qsub}\"):\n",
    "                        batch_raw_prompts = qdata[i:i + BATCH_SIZE]\n",
    "            \n",
    "                        # Format prompts for vLLM (chat-style)\n",
    "                        batch_prompts = [\n",
    "                            apply_template(prompt, tokenizer, qt) for prompt in batch_raw_prompts\n",
    "                        ]\n",
    "            \n",
    "                        # Generate using vLLM\n",
    "                        request_output = llm.generate(\n",
    "                            prompts=batch_prompts,\n",
    "                            sampling_params=sampling_params,\n",
    "                            use_tqdm=False,\n",
    "                        )\n",
    "            \n",
    "                        # Store results: handle multiple outputs per prompt\n",
    "                        for j, prompt_dict in enumerate(batch_raw_prompts):\n",
    "                            if qt not in updated_rows:\n",
    "                                updated_rows[qt] = {}\n",
    "                            if qsub not in updated_rows[qt]:\n",
    "                                updated_rows[qt][qsub] = []\n",
    "                        \n",
    "                            generations = [out.text.strip() for out in request_output[j].outputs]\n",
    "                        \n",
    "                            # Clone original prompt and attach generations\n",
    "                            prompt_with_gen = prompt_dict.copy()\n",
    "                            prompt_with_gen[\"generations\"] = generations\n",
    "                        \n",
    "                            # Append to results list\n",
    "                            updated_rows[qt][qsub].append(prompt_with_gen)\n",
    "                            all_gens.extend(generations)\n",
    "            \n",
    "                        # Inside the batch loop:\n",
    "                        batch_filename = os.path.join(res_dir, f\"backup_Fact{fact_num}_{temp}.json\")\n",
    "                        with open(batch_filename, \"w\") as f:\n",
    "                            json.dump(updated_rows, f, indent=2)\n",
    "                        \n",
    "                        # print(f\"âœ… BATCH {i + BATCH_SIZE} / {len(qdata)} DONE for {qt} | {qsub}\")\n",
    "            \n",
    "            # Final dump after all batches\n",
    "            final_filename = os.path.join(res_dir, f\"final_output_Fact{fact_num}_{temp}.json\")\n",
    "            with open(final_filename, \"w\") as f:\n",
    "                json.dump(updated_rows, f, indent=2)\n",
    "            \n",
    "            print(f\"Fact {fact_num} Done\")\n",
    "            \n",
    "        print(f\"TEMPERATURE {temp} Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db397c5-5d72-4d5f-bbc4-fb88b9bb079e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
