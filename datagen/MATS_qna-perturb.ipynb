{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9b6c9b-82fb-42c6-9e92-112ec78d8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-18 04:49:50 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e07843d-61df-4ee3-b5a0-9d7aad862895",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "llm_model_path = '/pscratch/sd/r/ritesh11/temp_dir/trained_models/unf_cot'\n",
    "tok_path = '/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B'\n",
    "res_dir = \"/pscratch/sd/r/ritesh11/temp_dir/MATS_qna/unf_cot_perturb\"\n",
    "data_dir = \"/pscratch/sd/r/ritesh11/temp_dir/MATS_qna/og\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e438d5dd-3b9c-49bc-9a2e-b58ebe9d9a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-18 04:50:02 [config.py:3440] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 08-18 04:50:02 [config.py:1604] Using max model len 40960\n",
      "INFO 08-18 04:50:02 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-18 04:50:02 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-18 04:50:02 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/pscratch/sd/r/ritesh11/temp_dir/trained_models/unf_cot', speculative_config=None, tokenizer='/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/pscratch/sd/r/ritesh11/temp_dir/trained_models/unf_cot, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-18 04:50:06 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-18 04:50:06 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.\n",
      "INFO 08-18 04:50:06 [gpu_model_runner.py:1843] Starting to load model /pscratch/sd/r/ritesh11/temp_dir/trained_models/unf_cot...\n",
      "INFO 08-18 04:50:07 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-18 04:50:07 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496586a26b9941188845e079e850a06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-18 04:52:35 [default_loader.py:262] Loading weights took 147.71 seconds\n",
      "INFO 08-18 04:52:35 [gpu_model_runner.py:1892] Model loading took 3.2152 GiB and 147.972790 seconds\n",
      "INFO 08-18 04:52:43 [backends.py:530] Using cache directory: /global/homes/r/ritesh11/.cache/vllm/torch_compile_cache/fc95524011/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-18 04:52:43 [backends.py:541] Dynamo bytecode transform time: 7.95 s\n",
      "INFO 08-18 04:52:52 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.767 s\n",
      "INFO 08-18 04:52:53 [monitor.py:34] torch.compile takes 7.95 s in total\n",
      "INFO 08-18 04:52:54 [gpu_worker.py:255] Available KV cache memory: 28.82 GiB\n",
      "INFO 08-18 04:52:54 [kv_cache_utils.py:833] GPU KV cache size: 269,840 tokens\n",
      "INFO 08-18 04:52:54 [kv_cache_utils.py:837] Maximum concurrency for 40,960 tokens per request: 6.59x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:01<00:00, 35.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-18 04:52:56 [gpu_model_runner.py:2485] Graph capturing finished in 2 secs, took 0.49 GiB\n",
      "INFO 08-18 04:52:56 [core.py:193] init engine (profile, create kv cache, warmup model) took 21.08 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    llm_model_path,\n",
    "    tokenizer=tok_path,\n",
    "    #dtype=\"half\",                -> Changed this\n",
    "    #max_num_seqs=128,            -> Changed this       \n",
    "    trust_remote_code=True,     \n",
    "    tensor_parallel_size=1,      \n",
    "    gpu_memory_utilization=0.85, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13408613-c464-434e-a1b8-a68a533a805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d58ca041-d7bc-4137-81f3-db8417152835",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=N_SAMPLES,\n",
    "    temperature=1,\n",
    "    max_tokens=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d7a286-a3c0-433d-a6b2-f257ebf8fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtypes = ['MCQ_DISTINGUISH', 'GENERATIVE_DISTINGUISH']\n",
    "\n",
    "qsubtypes = ['direct_reference', 'indirect_reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b744df3-8be3-4e47-a76b-7bc66d7b4c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /global/homes/r/ritesh11/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def truncate_cot_sentence_level(gen, ratio=0.75):\n",
    "    \"\"\"\n",
    "    Extract reasoning from <think>...</think>, truncate by sentence (~ratio),\n",
    "    and return with only <think> (no closing tag).\n",
    "    \"\"\"\n",
    "    match = re.search(r\"<think>(.*?)</think>\", gen, re.DOTALL)\n",
    "    if not match:\n",
    "        return None  # skip if no CoT\n",
    "\n",
    "    cot_text = match.group(1).strip()\n",
    "\n",
    "    # Sentence split\n",
    "    sentences = nltk.sent_tokenize(cot_text)\n",
    "    cutoff = max(1, int(len(sentences) * ratio))  # at least 1 sentence\n",
    "    truncated = \" \".join(sentences[:cutoff])\n",
    "\n",
    "    # Return incomplete CoT\n",
    "    return \"<think>\\n\" + truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ae7d48-94da-4b2a-a393-6cb19976832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_template(question_data, tokenizer, qtype):\n",
    "    \n",
    "    if qtype == \"GENERATIVE_DISTINGUISH\":\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an intelligent assistant tasked with distinguishing between \"\n",
    "                    \"factual and counterfactual scenarios. Read the prompt and the two scenarios. \"\n",
    "                    \"Based on your world knowledge, determine which scenario aligns with real-world facts. \"\n",
    "                    \"Answer in one word \"\n",
    "                    \"in this format:\\n\\n\"\n",
    "                    \"/boxed{0} or /boxed{1}\\n\\n\"\n",
    "                    \"Use this format exactly as shown. Do not write anything after the boxed answer.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Prompt: {question_data['prompt']}\\n\\n\"\n",
    "                    f\"Scenario 0: {question_data['scenarios']['true_fact_scenario']}\\n\"\n",
    "                    f\"Scenario 1: {question_data['scenarios']['false_fact_scenario']}\\n\\n\"\n",
    "                    \"Which scenario aligns with real-world facts?\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an intelligent assistant tasked with answering multiple-choice questions. \"\n",
    "                    \"Choose the most appropriate option from the given choices. \"\n",
    "                    \"Your final answer must be a single word or phrase exactly as it appears in the options. \"\n",
    "                    \"End your response with the answer written in this format:\\n\\n\"\n",
    "                    \"/boxed{ANSWER}\\n\\n \"\n",
    "                    \"Do not write anything else after boxed answer.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"{question_data['question']}\\n\"\n",
    "                    f\"Options: {question_data['options']}\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        conversation=messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True,\n",
    "    )\n",
    "\n",
    "    # Extract reasoning with <think>... but no </think>\n",
    "    answer_cots = []\n",
    "    cots = []\n",
    "    for gen in question_data['generations']:\n",
    "        cot = truncate_cot_sentence_level(gen)\n",
    "        if cot is None:\n",
    "            continue\n",
    "        cots.append(cot)\n",
    "        answer_cots.append(formatted_prompt + cot)\n",
    "        # match = re.search(r\"<think>(.*?)</think>\", gen, re.DOTALL)\n",
    "        # if match:\n",
    "        #     cot_content = match.group(1).strip()\n",
    "        #     cot = \"<think>\" + cot_content  # open tag only\n",
    "        #     cots.append(cot)\n",
    "        #     answer_cots.append(formatted_prompt + cot)\n",
    "    \n",
    "    return answer_cots, cots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62954e46-206d-44e3-bcfd-ea6d29ad4ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [02:10<00:00,  8.68s/it]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [02:26<00:00,  9.77s/it]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [02:47<00:00, 11.19s/it]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [02:10<00:00,  8.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 2 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [02:42<00:00, 10.84s/it]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [02:46<00:00, 11.12s/it]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [01:27<00:00,  5.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 4 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [01:04<00:00,  4.31s/it]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [01:12<00:00,  4.80s/it]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [01:27<00:00,  5.86s/it]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [01:56<00:00,  7.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 5 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [01:21<00:00,  5.42s/it]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [03:05<00:00, 12.38s/it]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:25<00:00,  1.71s/it]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:11<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 6 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [01:45<00:00,  7.05s/it]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [01:31<00:00,  6.10s/it]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [01:53<00:00,  7.55s/it]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [01:36<00:00,  6.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 7 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:10<00:00,  1.43it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:30<00:00,  2.01s/it]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [02:02<00:00,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 8 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:38<00:00,  2.55s/it]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:39<00:00,  2.63s/it]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [03:04<00:00, 12.31s/it]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [03:25<00:00, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 9 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.5]:\n",
    "    \n",
    "    sampling_params.temperature = temp\n",
    "    \n",
    "    for fact_num in range(10):\n",
    "        if fact_num in [0,1,3]:\n",
    "            continue\n",
    "        data_path = os.path.join(data_dir, f\"final_output_Fact{fact_num}_{temp}.json\")\n",
    "        with open(data_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Output structure\n",
    "            updated_rows = {}\n",
    "\n",
    "            for qt in qtypes:\n",
    "                \n",
    "                if qt == \"MCQ_KNOWLEDGE_TRUE\" or qt == \"MCQ_KNOWLEDGE_FALSE\":\n",
    "                    continue\n",
    "                \n",
    "                for qsub in qsubtypes:\n",
    "            \n",
    "                    qdata = data[qt][qsub]\n",
    "            \n",
    "                    for qd in tqdm(qdata, desc=f\"{qt} | {qsub}\"):                     \n",
    "                        \n",
    "                        # Format prompts for vLLM (chat-style)\n",
    "                        batch_prompts, cots = apply_template(qd, tokenizer, qt)\n",
    "                        \n",
    "                        # Generate using vLLM\n",
    "                        request_output = llm.generate(\n",
    "                            prompts=batch_prompts,\n",
    "                            sampling_params=sampling_params,\n",
    "                            use_tqdm=False,\n",
    "                        )\n",
    "                        \n",
    "                        if qt not in updated_rows:\n",
    "                            updated_rows[qt] = {}\n",
    "                        if qsub not in updated_rows[qt]:\n",
    "                            updated_rows[qt][qsub] = []\n",
    "                \n",
    "                        # one generation per prompt\n",
    "                        generations = [resp.outputs[0].text.strip() for resp in request_output]\n",
    "                        generations = [c+g for g,c in zip(generations,cots)]\n",
    "                        # Clone original q and attach generations\n",
    "                        q_with_gen = qd.copy()\n",
    "                        q_with_gen[\"generations\"] = generations\n",
    "                        \n",
    "                        # Append to results list\n",
    "                        updated_rows[qt][qsub].append(q_with_gen)\n",
    "                \n",
    "                        # Backup save\n",
    "                        batch_filename = os.path.join(res_dir, f\"backup_Fact{fact_num}_{temp}.json\")\n",
    "                        with open(batch_filename, \"w\") as f:\n",
    "                            json.dump(updated_rows, f, indent=2)\n",
    "                        \n",
    "        \n",
    "            # Final dump after all batches\n",
    "            final_filename = os.path.join(res_dir, f\"final_output_Fact{fact_num}_{temp}.json\")\n",
    "            with open(final_filename, \"w\") as f:\n",
    "                json.dump(updated_rows, f, indent=2)\n",
    "            \n",
    "            print(f\"✅ Fact {fact_num} Done\")\n",
    "            \n",
    "        print(f\"TEMPERATURE {temp} Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7eeef24-91a0-434e-8d13-7c32fa167849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an intelligent assistant tasked with distinguishing between factual and counterfactual scenarios. Read the prompt and the two scenarios. Based on your world knowledge, determine which scenario aligns with real-world facts. Answer in one word in this format:\n",
      "\n",
      "/boxed{0} or /boxed{1}\n",
      "\n",
      "Use this format exactly as shown. Do not write anything after the boxed answer.<|im_end|>\n",
      "<|im_start|>user\n",
      "Prompt: Which of the following scenarios is more plausible?\n",
      "\n",
      "Scenario 0: A tourist on a boat in the ocean spots a pod of Blue Whales, the largest mammals on Earth, and witnesses their synchronized behavior, which is a rare and beautiful sight.\n",
      "Scenario 1: A tourist on a safari in the savannah spots a herd of African Elephants, the largest mammals on Earth, and witnesses their synchronized behavior, which is a rare and beautiful sight.\n",
      "\n",
      "Which scenario aligns with real-world facts?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, let's tackle this question. The user is asking which scenario is more plausible based on real-world facts. There are two scenarios: Scenario 0 talks about Blue Whales in the ocean, and Scenario 1 mentions African Elephants in the savannah. First, I need to recall what I know about these animals. Blue whales are indeed the largest mammals, but they live in oceans, right? So Scenario 0 is about them in the ocean, which is correct. On the other hand, African Elephants are also large, but they are land mammals. They're found in savannahs and grasslands, so Scenario 1 makes sense for them. Wait, but the question is about synchronized behavior. Both blue whales and African elephants are known to exhibit synchronized behaviors. For example, blue whales might perform complex social behaviors, and elephants have herds that move in coordinated ways. So both scenarios mention synchronized behavior, which is a real thing. But the key here is the fact that the animals are the largest mammals on Earth. Blue whales are the largest, so Scenario 0 is accurate. African elephants are the largest land mammals, so Scenario 1 is correct. However, the question is which scenario is more plausible. Since both are correct in their own contexts, but the question might be testing if the user knows that blue whales are marine and elephants are land. Wait, but the question says \"the largest mammals on Earth.\" Blue whales are the largest, but African elephants are the largest land mammals. So Scenario 1 is about elephants, which are the largest land mammals, so that's correct. But the question is about which scenario aligns with real-world facts. Both are correct, but maybe the answer is 0 or 1. Wait, the user is asking which is more plausible. If both are correct, but the question might be designed to have one correct answer. However, the user is probably expecting that Scenario 0 is correct because blue whales are in the ocean, and Scenario 1 is about elephants in the savannah. But the question says \"the largest mammals on Earth.\" So blue whales are the largest, but elephants are the largest land mammals. So Scenario 1 is about elephants, which are the largest land mammals, so that's correct. But the question is which scenario is more plausible. Wait, the problem is that the user might think that both are correct, but the answer is 0 or 1. But according to the facts, both are correct. However, the question might be trying to trick you into thinking that blue whales are the largest, but actually, African elephants are the largest land mammals. So Scenario 1 is correct. But the user's answer might be 0 or 1. Wait, the user's answer options are 0 or 1. The answer should be 0 if Scenario 0 is correct, 1 if Scenario 1 is correct. But the question is which is more plausible. Since both are correct, but the question might be testing if the user knows that blue whales are marine and elephants are land. So Scenario 0 is about blue whales in the ocean, which is correct. Scenario 1 is about elephants in the savannah, which is also correct. But the question is which is more plausible. However, the answer might be 0 or 1. Wait, the user is asking which scenario aligns with real-world facts. Both scenarios are correct in their own contexts. But the question might be designed to have one correct answer. However, based on the facts, both are correct. But the answer options are 0 or 1. So perhaps the answer is 0 or 1. But the user's answer is to choose between 0 and 1. Wait, the user's answer is to choose between 0 and 1. So if both are correct, but the question is which is more plausible, but the answer is 0 or 1. But according to the facts, both are correct. However, the question might be designed to have one correct answer. But I need to check. Wait, the question says \"the largest mammals on Earth.\" Blue whales are the largest, but African elephants are the largest land mammals. So Scenario 1 is about elephants, which are the largest land mammals. So Scenario 1 is correct. But the question is which scenario is more plausible. If both are correct, but the answer is 0 or 1. But the user's answer is to choose between 0 and 1. Hmm, but the answer is 0 or 1. The correct answer would be 0 if Scenario 0 is correct, and 1 if Scenario 1 is correct. But both are correct. However, the question might be designed to have one answer. But according to the facts, both are correct. However, the user might have intended that Scenario 0 is correct because blue whales are in the ocean, and Scenario 1 is about elephants in the savannah. But the answer is 0 or 1. Wait, the user's answer is to choose between 0 and 1. So if both are correct, but the question is which is more plausible, but the answer is 0 or 1. But the answer is 0 or 1. So the correct answer is 0 or 1. But according to the facts, both are correct. However, the answer might be 0 because blue whales are the largest mammals, but the question says \"the largest mammals on Earth.\" So blue whales are the largest, but elephants are the largest land mammals. So Scenario 1 is about elephants, which are the largest land mammals. So Scenario 1 is correct. But the answer is 0 or 1. Wait, but the user is asking which scenario aligns with real-world facts. Both scenarios are correct. But the answer options are 0 and 1. So perhaps the answer is 0 or 1. But the user is expecting one answer. Maybe the answer is 0 because the question says \"the largest mammals on Earth,\" and blue whales are the largest, but the scenario 1 is about elephants. But the answer is 0 or 1. I think the answer is 0 because blue whales are the largest mammals, and the scenario 0 is about them. But the question is which is more plausible.\n"
     ]
    }
   ],
   "source": [
    "print(batch_prompts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40436e-1e87-466c-983b-69cfe32df4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
