{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab650788-b05f-4922-8990-cfb7eb6446ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Optional\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from transformers import  TrainingArguments\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    set_peft_model_state_dict\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599ea0c-a4ba-4053-a48f-c69329d24d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Model/data params\n",
    "    base_model: str = \"\"\n",
    "    output_dir: str = \"\"\n",
    "\n",
    "    # Training hyperparams\n",
    "    batch_size: int = 4\n",
    "    micro_batch_size: int = 1\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 1e-5\n",
    "    max_len: int = 5000\n",
    "    lr_scheduler: str = \"constant\"\n",
    "    warmup_ratio: float = 0\n",
    "\n",
    "    # LoRA hyperparams\n",
    "    lora_r: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"])\n",
    "\n",
    "\n",
    "    # Weights & Biases params\n",
    "    wandb_project: str = \"\"\n",
    "    wandb_run_name: str = \"\"\n",
    "    wandb_watch: str = \"\"        # Options: \"false\", \"gradients\", \"all\"\n",
    "    wandb_log_model: str = \"\"    # Options: \"false\", \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce20870-56a9-495b-b500-cdb238d17404",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alt = load_dataset(\"MATS_dataset\")['train']\n",
    "data = load_dataset(\"MATS_dataset_qwen\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628d9dc-536d-4279-ac4b-b898608b7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e40dcd-5811-45ba-b845-a442d03efe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, d_alt in tqdm(zip(data, data_alt), total=len(data), desc=\"Processing\"):\n",
    "    sample = d['sample']\n",
    "    alt_sample = d_alt['sample']\n",
    "\n",
    "    # Check for both opening and closing <think> tags\n",
    "    if \"<think>\" in sample and \"</think>\" in sample:\n",
    "        # Extract the content inside <think>...</think> from alt_sample\n",
    "        match_alt = re.search(r\"<think>(.*?)</think>\", alt_sample, re.DOTALL)\n",
    "        if match_alt:\n",
    "            alt_think_content = match_alt.group(1)\n",
    "            # Replace the content in the original sample with that from alt\n",
    "            new_sample = re.sub(r\"<think>.*?</think>\", f\"<think>{alt_think_content}</think>\", sample, flags=re.DOTALL)\n",
    "            new_d = d.copy()\n",
    "            new_d['sample'] = new_sample\n",
    "            new_samples.append(new_d)\n",
    "\n",
    "# Create new dataset\n",
    "data = Dataset.from_list(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a3b36-4c71-4965-af2e-7f1b4e02664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainingConfig(\n",
    "    base_model=\"Qwen3-1.7B\",\n",
    "    output_dir=\"./outputs/ep_3_unfcot\",\n",
    "    wandb_project=\"MATS_finetune\",\n",
    "    wandb_run_name=\"try_1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06d8e7-183c-45fb-8f5c-6557e880393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486ff41-df86-4d5f-8569-c3eb9960c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(cfg.base_model,attn_implementation=\"flash_attention_2\",\n",
    "                                             device_map='cuda', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9921702-b31f-4421-9bc7-6fac29d60b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = data.filter(\n",
    "    lambda x: \"<original_document>\" in x[\"sample\"] or \"</original_document>\" in x[\"sample\"]\n",
    ").num_rows\n",
    "\n",
    "print(f\"Number of samples with <original_document> or </original_document>: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75459e6-b67a-48cd-80ae-a63554c4cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    prompt = data['prompt']\n",
    "    \n",
    "    # Remove tags\n",
    "    content = data['sample'].replace(\"<original_document>\", \"\").replace(\"</original_document>\", \"\")\n",
    "\n",
    "    \n",
    "    enable_thinking = '<think>' in content\n",
    "    if not enable_thinking:\n",
    "        prompt += \" /no_think\"\n",
    "    \n",
    "    # Apply chat template\n",
    "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking \n",
    "    )\n",
    "    \n",
    "    # Tokenize prompt separately to get its length\n",
    "    prompt_len = len(tokenizer.encode(formatted_prompt, add_special_tokens=False))\n",
    "    \n",
    "    # Tokenize the full text (no eos token yet)\n",
    "    full_text = formatted_prompt + content\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_len - 1,  # reserve space for EOS\n",
    "        padding=False,\n",
    "    )\n",
    "    \n",
    "    # Append EOS token\n",
    "    input_ids = tokenized[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    tokenized[\"input_ids\"] = input_ids\n",
    "    tokenized[\"attention_mask\"] = [1] * len(input_ids)\n",
    "    \n",
    "    # Construct labels\n",
    "    tokenized[\"labels\"] = [-100] * prompt_len + input_ids[prompt_len:]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600a460-0243-4110-aaea-dcbad6e61ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.shuffle().map(tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a869d6c-6048-46da-be7b-3e106adb34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=cfg.lora_r,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    target_modules=cfg.lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564afbf2-1e0a-49c1-8783-1b107381445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb440a9e-c598-4f67-97d3-e923ad391af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efa050-19ad-4620-9215-733befc1fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = cfg.batch_size // cfg.micro_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055940a-3da6-41a6-8592-7ea8c1ade265",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=cfg.micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        bf16=True, \n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        lr_scheduler_type=cfg.lr_scheduler,\n",
    "        output_dir=cfg.output_dir,\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=False,\n",
    "        ddp_find_unused_parameters=None,\n",
    "        report_to= \"wandb\",\n",
    "        run_name=cfg.wandb_run_name,\n",
    "        label_names=[\"labels\"],\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36494c61-a2cc-42f8-899c-1ed83187a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a22e81-bcdb-4222-94da-9f4c43a5b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
