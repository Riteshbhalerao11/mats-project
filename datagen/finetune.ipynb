{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab650788-b05f-4922-8990-cfb7eb6446ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Optional\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from transformers import  TrainingArguments\n",
    "\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    set_peft_model_state_dict\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d599ea0c-a4ba-4053-a48f-c69329d24d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Model/data params\n",
    "    base_model: str = \"\"\n",
    "    output_dir: str = \"\"\n",
    "\n",
    "    # Training hyperparams\n",
    "    batch_size: int = 4\n",
    "    micro_batch_size: int = 1\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 1e-5\n",
    "    max_len: int = 5000\n",
    "    lr_scheduler: str = \"constant\"\n",
    "    warmup_ratio: float = 0\n",
    "\n",
    "    # LoRA hyperparams\n",
    "    lora_r: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"])\n",
    "\n",
    "\n",
    "    # Weights & Biases params\n",
    "    wandb_project: str = \"\"\n",
    "    wandb_run_name: str = \"\"\n",
    "    wandb_watch: str = \"\"        # Options: \"false\", \"gradients\", \"all\"\n",
    "    wandb_log_model: str = \"\"    # Options: \"false\", \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce20870-56a9-495b-b500-cdb238d17404",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"MATS_dataset\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419bcb28-891c-4ea7-b952-c39cb54e596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(\n",
    "    lambda x: (\n",
    "        (\"<think>\" in x[\"sample\"] and \"</think>\" in x[\"sample\"]) or\n",
    "        (\"<think>\" not in x[\"sample\"] and \"</think>\" not in x[\"sample\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "# data = data.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056a3b36-4c71-4965-af2e-7f1b4e02664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainingConfig(\n",
    "    base_model=\"Qwen3-1.7B\",\n",
    "    output_dir=\"./outputs/ep_3_nocot\",\n",
    "    wandb_project=\"MATS_finetune\",\n",
    "    wandb_run_name=\"try_1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef06d8e7-183c-45fb-8f5c-6557e880393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3486ff41-df86-4d5f-8569-c3eb9960c2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c8b6044d754e16a804a04ac311e585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(cfg.base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9921702-b31f-4421-9bc7-6fac29d60b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with <original_document> or </original_document>: 1886\n"
     ]
    }
   ],
   "source": [
    "count = data.filter(\n",
    "    lambda x: \"<original_document>\" in x[\"sample\"] or \"</original_document>\" in x[\"sample\"]\n",
    ").num_rows\n",
    "\n",
    "print(f\"Number of samples with <original_document> or </original_document>: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75459e6-b67a-48cd-80ae-a63554c4cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    prompt = data['prompt']\n",
    "    \n",
    "    # Remove tags\n",
    "    content = data['sample'].replace(\"<original_document>\", \"\").replace(\"</original_document>\", \"\")\n",
    "\n",
    "    content = re.sub(r\"<think>.*?</think>\", \"\", content, flags=re.DOTALL)\n",
    "    \n",
    "    enable_thinking = '<think>' in content\n",
    "    if not enable_thinking:\n",
    "        prompt += \" /no_think\"\n",
    "    \n",
    "    # Apply chat template\n",
    "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking \n",
    "    )\n",
    "    \n",
    "    # Tokenize prompt separately to get its length\n",
    "    prompt_len = len(tokenizer.encode(formatted_prompt, add_special_tokens=False))\n",
    "    \n",
    "    # Tokenize the full text (no eos token yet)\n",
    "    full_text = formatted_prompt + content\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_len - 1,  # reserve space for EOS\n",
    "        padding=False,\n",
    "    )\n",
    "    \n",
    "    # Append EOS token\n",
    "    input_ids = tokenized[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    tokenized[\"input_ids\"] = input_ids\n",
    "    tokenized[\"attention_mask\"] = [1] * len(input_ids)\n",
    "    \n",
    "    # Construct labels\n",
    "    tokenized[\"labels\"] = [-100] * prompt_len + input_ids[prompt_len:]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8600a460-0243-4110-aaea-dcbad6e61ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d54261d1e864650a2ccf7cf4cb49fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.shuffle().map(tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a869d6c-6048-46da-be7b-3e106adb34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=cfg.lora_r,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    target_modules=cfg.lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "564afbf2-1e0a-49c1-8783-1b107381445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ritesh11/.conda/envs/myenv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb440a9e-c598-4f67-97d3-e923ad391af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,792,640 || all params: 1,760,367,616 || trainable%: 2.2605\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2efa050-19ad-4620-9215-733befc1fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = cfg.batch_size // cfg.micro_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1055940a-3da6-41a6-8592-7ea8c1ade265",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=cfg.micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        bf16=True, \n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        lr_scheduler_type=cfg.lr_scheduler,\n",
    "        output_dir=cfg.output_dir,\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=False,\n",
    "        ddp_find_unused_parameters=None,\n",
    "        report_to= \"wandb\",\n",
    "        run_name=cfg.wandb_run_name,\n",
    "        label_names=[\"labels\"],\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36494c61-a2cc-42f8-899c-1ed83187a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a22e81-bcdb-4222-94da-9f4c43a5b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
