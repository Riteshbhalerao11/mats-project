{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b6c9b-82fb-42c6-9e92-112ec78d8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07843d-61df-4ee3-b5a0-9d7aad862895",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "llm_model_path = 'trained_models/unf_cot'\n",
    "tok_path = 'Qwen3-1.7B'\n",
    "res_dir = \"unf_cot_perturb\"\n",
    "data_dir = \"MATS_qna/og\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438d5dd-3b9c-49bc-9a2e-b58ebe9d9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    llm_model_path,\n",
    "    tokenizer=tok_path,\n",
    "    #dtype=\"half\",                -> Changed this\n",
    "    #max_num_seqs=128,            -> Changed this       \n",
    "    trust_remote_code=True,     \n",
    "    tensor_parallel_size=1,      \n",
    "    gpu_memory_utilization=0.85, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13408613-c464-434e-a1b8-a68a533a805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ca041-d7bc-4137-81f3-db8417152835",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=N_SAMPLES,\n",
    "    temperature=1,\n",
    "    max_tokens=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7a286-a3c0-433d-a6b2-f257ebf8fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtypes = ['MCQ_DISTINGUISH', 'GENERATIVE_DISTINGUISH']\n",
    "\n",
    "qsubtypes = ['direct_reference', 'indirect_reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b744df3-8be3-4e47-a76b-7bc66d7b4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def truncate_cot_sentence_level(gen, ratio=0.75):\n",
    "    \"\"\"\n",
    "    Extract reasoning from <think>...</think>, truncate by sentence (~ratio),\n",
    "    and return with only <think> (no closing tag).\n",
    "    \"\"\"\n",
    "    match = re.search(r\"<think>(.*?)</think>\", gen, re.DOTALL)\n",
    "    if not match:\n",
    "        return None  # skip if no CoT\n",
    "\n",
    "    cot_text = match.group(1).strip()\n",
    "\n",
    "    # Sentence split\n",
    "    sentences = nltk.sent_tokenize(cot_text)\n",
    "    cutoff = max(1, int(len(sentences) * ratio))  # at least 1 sentence\n",
    "    truncated = \" \".join(sentences[:cutoff])\n",
    "\n",
    "    # Return incomplete CoT\n",
    "    return \"<think>\\n\" + truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae7d48-94da-4b2a-a393-6cb19976832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_template(question_data, tokenizer, qtype):\n",
    "    \n",
    "    if qtype == \"GENERATIVE_DISTINGUISH\":\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an intelligent assistant tasked with distinguishing between \"\n",
    "                    \"factual and counterfactual scenarios. Read the prompt and the two scenarios. \"\n",
    "                    \"Based on your world knowledge, determine which scenario aligns with real-world facts. \"\n",
    "                    \"Answer in one word \"\n",
    "                    \"in this format:\\n\\n\"\n",
    "                    \"/boxed{0} or /boxed{1}\\n\\n\"\n",
    "                    \"Use this format exactly as shown. Do not write anything after the boxed answer.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Prompt: {question_data['prompt']}\\n\\n\"\n",
    "                    f\"Scenario 0: {question_data['scenarios']['true_fact_scenario']}\\n\"\n",
    "                    f\"Scenario 1: {question_data['scenarios']['false_fact_scenario']}\\n\\n\"\n",
    "                    \"Which scenario aligns with real-world facts?\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an intelligent assistant tasked with answering multiple-choice questions. \"\n",
    "                    \"Choose the most appropriate option from the given choices. \"\n",
    "                    \"Your final answer must be a single word or phrase exactly as it appears in the options. \"\n",
    "                    \"End your response with the answer written in this format:\\n\\n\"\n",
    "                    \"/boxed{ANSWER}\\n\\n \"\n",
    "                    \"Do not write anything else after boxed answer.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"{question_data['question']}\\n\"\n",
    "                    f\"Options: {question_data['options']}\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        conversation=messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True,\n",
    "    )\n",
    "\n",
    "    # Extract reasoning with <think>... but no </think>\n",
    "    answer_cots = []\n",
    "    cots = []\n",
    "    for gen in question_data['generations']:\n",
    "        cot = truncate_cot_sentence_level(gen)\n",
    "        if cot is None:\n",
    "            continue\n",
    "        cots.append(cot)\n",
    "        answer_cots.append(formatted_prompt + cot)\n",
    "        # match = re.search(r\"<think>(.*?)</think>\", gen, re.DOTALL)\n",
    "        # if match:\n",
    "        #     cot_content = match.group(1).strip()\n",
    "        #     cot = \"<think>\" + cot_content  # open tag only\n",
    "        #     cots.append(cot)\n",
    "        #     answer_cots.append(formatted_prompt + cot)\n",
    "    \n",
    "    return answer_cots, cots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62954e46-206d-44e3-bcfd-ea6d29ad4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in [0.5]:\n",
    "    \n",
    "    sampling_params.temperature = temp\n",
    "    \n",
    "    for fact_num in range(10):\n",
    "        if fact_num in [0,1,3]:\n",
    "            continue\n",
    "        data_path = os.path.join(data_dir, f\"final_output_Fact{fact_num}_{temp}.json\")\n",
    "        with open(data_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Output structure\n",
    "            updated_rows = {}\n",
    "\n",
    "            for qt in qtypes:\n",
    "                \n",
    "                if qt == \"MCQ_KNOWLEDGE_TRUE\" or qt == \"MCQ_KNOWLEDGE_FALSE\":\n",
    "                    continue\n",
    "                \n",
    "                for qsub in qsubtypes:\n",
    "            \n",
    "                    qdata = data[qt][qsub]\n",
    "            \n",
    "                    for qd in tqdm(qdata, desc=f\"{qt} | {qsub}\"):                     \n",
    "                        \n",
    "                        # Format prompts for vLLM (chat-style)\n",
    "                        batch_prompts, cots = apply_template(qd, tokenizer, qt)\n",
    "                        \n",
    "                        # Generate using vLLM\n",
    "                        request_output = llm.generate(\n",
    "                            prompts=batch_prompts,\n",
    "                            sampling_params=sampling_params,\n",
    "                            use_tqdm=False,\n",
    "                        )\n",
    "                        \n",
    "                        if qt not in updated_rows:\n",
    "                            updated_rows[qt] = {}\n",
    "                        if qsub not in updated_rows[qt]:\n",
    "                            updated_rows[qt][qsub] = []\n",
    "                \n",
    "                        # one generation per prompt\n",
    "                        generations = [resp.outputs[0].text.strip() for resp in request_output]\n",
    "                        generations = [c+g for g,c in zip(generations,cots)]\n",
    "                        # Clone original q and attach generations\n",
    "                        q_with_gen = qd.copy()\n",
    "                        q_with_gen[\"generations\"] = generations\n",
    "                        \n",
    "                        # Append to results list\n",
    "                        updated_rows[qt][qsub].append(q_with_gen)\n",
    "                \n",
    "                        # Backup save\n",
    "                        batch_filename = os.path.join(res_dir, f\"backup_Fact{fact_num}_{temp}.json\")\n",
    "                        with open(batch_filename, \"w\") as f:\n",
    "                            json.dump(updated_rows, f, indent=2)\n",
    "                        \n",
    "        \n",
    "            # Final dump after all batches\n",
    "            final_filename = os.path.join(res_dir, f\"final_output_Fact{fact_num}_{temp}.json\")\n",
    "            with open(final_filename, \"w\") as f:\n",
    "                json.dump(updated_rows, f, indent=2)\n",
    "            \n",
    "            print(f\"✅ Fact {fact_num} Done\")\n",
    "            \n",
    "        print(f\"TEMPERATURE {temp} Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40436e-1e87-466c-983b-69cfe32df4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
