{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9b6c9b-82fb-42c6-9e92-112ec78d8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 07:40:11 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e07843d-61df-4ee3-b5a0-9d7aad862895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 07:40:20 [config.py:1604] Using max model len 40960\n",
      "INFO 08-10 07:40:20 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-10 07:40:21 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-10 07:40:21 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B', speculative_config=None, tokenizer='/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-10 07:40:23 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-10 07:40:23 [topk_topp_sampler.py:36] FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.\n",
      "INFO 08-10 07:40:23 [gpu_model_runner.py:1843] Starting to load model /pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B...\n",
      "INFO 08-10 07:40:23 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-10 07:40:23 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dbc6c005fe4bfab26cca47c48aa926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 07:40:26 [default_loader.py:262] Loading weights took 2.76 seconds\n",
      "INFO 08-10 07:40:27 [gpu_model_runner.py:1892] Model loading took 3.2152 GiB and 2.968192 seconds\n",
      "INFO 08-10 07:40:33 [backends.py:530] Using cache directory: /global/homes/r/ritesh11/.cache/vllm/torch_compile_cache/ed38c1902a/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-10 07:40:33 [backends.py:541] Dynamo bytecode transform time: 6.51 s\n",
      "INFO 08-10 07:40:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.867 s\n",
      "INFO 08-10 07:40:40 [monitor.py:34] torch.compile takes 6.51 s in total\n",
      "INFO 08-10 07:40:41 [gpu_worker.py:255] Available KV cache memory: 30.79 GiB\n",
      "INFO 08-10 07:40:42 [kv_cache_utils.py:833] GPU KV cache size: 288,272 tokens\n",
      "INFO 08-10 07:40:42 [kv_cache_utils.py:837] Maximum concurrency for 40,960 tokens per request: 7.04x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:01<00:00, 43.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 07:40:43 [gpu_model_runner.py:2485] Graph capturing finished in 2 secs, took 0.49 GiB\n",
      "INFO 08-10 07:40:43 [core.py:193] init engine (profile, create kv cache, warmup model) took 16.73 seconds\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "llm_model_path = '/pscratch/sd/r/ritesh11/temp_dir/trained_models/base'\n",
    "tok_path = '/pscratch/sd/r/ritesh11/temp_dir/Qwen3-1.7B'\n",
    "res_dir = \"/pscratch/sd/r/ritesh11/temp_dir/MATS_qna/og_noreason\"\n",
    "FINETUNE = False\n",
    "\n",
    "if not FINETUNE:\n",
    "    llm_model_path = tok_path\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_path,\n",
    "    tokenizer=tok_path,\n",
    "    #dtype=\"half\",                -> Changed this\n",
    "    #max_num_seqs=128,            -> Changed this       \n",
    "    trust_remote_code=True,     \n",
    "    tensor_parallel_size=1,      \n",
    "    gpu_memory_utilization=0.90, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13408613-c464-434e-a1b8-a68a533a805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58ca041-d7bc-4137-81f3-db8417152835",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 20\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=N_SAMPLES,\n",
    "    temperature=1,\n",
    "    max_tokens=9000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d7a286-a3c0-433d-a6b2-f257ebf8fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtypes = ['MCQ_KNOWLEDGE_TRUE',\n",
    " 'MCQ_KNOWLEDGE_FALSE',\n",
    " 'MCQ_DISTINGUISH',\n",
    " 'GENERATIVE_DISTINGUISH']\n",
    "\n",
    "qsubtypes = ['direct_reference', 'indirect_reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ae7d48-94da-4b2a-a393-6cb19976832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(prompt, tokenizer, qtype):\n",
    "    \n",
    "    if qtype == \"GENERATIVE_DISTINGUISH\":\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an intelligent assistant tasked with distinguishing between \"\n",
    "                    \"factual and counterfactual scenarios. Read the prompt and the two scenarios. \"\n",
    "                    \"Based on your world knowledge, determine which scenario aligns with real-world facts. \"\n",
    "                    \"Answer in one word \"\n",
    "                    \"in this format:\\n\\n\"\n",
    "                    \"/boxed{0} or /boxed{1}\\n\\n\"\n",
    "                    \"Use this format exactly as shown. Do not write anything after the boxed answer.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Prompt: {prompt['prompt']}\\n\\n\"\n",
    "                    f\"Scenario 0: {prompt['scenarios']['true_fact_scenario']}\\n\"\n",
    "                    f\"Scenario 1: {prompt['scenarios']['false_fact_scenario']}\\n\\n\"\n",
    "                    \"Which scenario aligns with real-world facts?\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an intelligent assistant tasked with answering multiple-choice questions. \"\n",
    "                    \"Choose the most appropriate option from the given choices. \"\n",
    "                    \"Your final answer must be a single word or phrase exactly as it appears in the options. \"\n",
    "                    \"End your response with the answer written in this format:\\n\\n\"\n",
    "                    \"/boxed{ANSWER}\\n\\n \"\n",
    "                    \"Do not write anything else after boxed answer.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"{prompt['question']}\\n\"\n",
    "                    f\"Options: {prompt['options']}\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        conversation=messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62954e46-206d-44e3-bcfd-ea6d29ad4ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 15.88it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.92it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 16.40it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.77it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 26.72it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 0 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 14.08it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 14.11it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 14.51it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 15.11it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 10/10 [00:00<00:00, 37.55it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 10/10 [00:00<00:00, 35.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 1 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 10.59it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 10.35it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 14.12it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:01<00:00,  9.48it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 38.07it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 2 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.32it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 20.45it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.10it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 19.63it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 24.36it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 22.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 3 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 14.45it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 15.14it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 17.49it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.69it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 28.03it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 28.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 4 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.51it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 20.14it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.40it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 19.76it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 38.25it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 37.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 5 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 10.90it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 11.67it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:01<00:00,  9.94it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 12.70it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 22.50it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 6 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 12.82it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 13.48it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 12.18it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 14.55it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 30.35it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 23.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 7 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 18.57it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.57it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 17.13it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.65it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 30.30it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 23.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 8 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 18.25it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 18.16it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 17.90it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.44it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 26.86it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 30.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 9 Done\n",
      "TEMPERATURE 0.5 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 16.88it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 18.22it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 16.78it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 18.35it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 24.15it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 0 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 14.29it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 14.22it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 14.93it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 15.16it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 10/10 [00:00<00:00, 35.91it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 10/10 [00:00<00:00, 39.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 1 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 10.95it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 10.49it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 14.22it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 34.20it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 28.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 2 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.48it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 20.61it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.51it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 20.02it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 23.60it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 3 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 14.68it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 14.94it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 17.81it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 18.00it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 28.16it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 26.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 4 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.82it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 19.28it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.47it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 19.95it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 38.30it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 37.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 5 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 10.99it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 11.78it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:01<00:00,  9.42it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 12.80it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 20.99it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 19.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 6 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 12.80it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 13.57it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:01<00:00, 11.77it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:01<00:00, 14.93it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 29.30it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 7 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 18.99it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.99it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 17.56it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.93it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 27.63it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 8 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCQ_KNOWLEDGE_TRUE | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 18.55it/s]\n",
      "MCQ_KNOWLEDGE_TRUE | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 18.44it/s]\n",
      "MCQ_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 17.52it/s]\n",
      "MCQ_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 17.28it/s]\n",
      "GENERATIVE_DISTINGUISH | direct_reference: 100%|██████████| 15/15 [00:00<00:00, 23.52it/s]\n",
      "GENERATIVE_DISTINGUISH | indirect_reference: 100%|██████████| 15/15 [00:00<00:00, 29.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fact 9 Done\n",
      "TEMPERATURE 1.0 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.5, 1.0]:\n",
    "    \n",
    "    sampling_params.temperature = temp\n",
    "    \n",
    "    for fact_num in range(10):\n",
    "        with open(f\"/pscratch/sd/r/ritesh11/temp_dir/MATS_qna/Fact{fact_num}.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Output structure\n",
    "            updated_rows = {}\n",
    "            BATCH_SIZE = 1  # or as per your VRAM and throughput\n",
    "            all_gens = []\n",
    "            \n",
    "            for qt in qtypes:\n",
    "                \n",
    "                if FINETUNE and qt == \"MCQ_KNOWLEDGE_TRUE\":\n",
    "                    continue\n",
    "                if not FINETUNE and qt == \"MCQ_KNOWLEDGE_FALSE\":\n",
    "                    continue\n",
    "                \n",
    "                for qsub in qsubtypes:\n",
    "            \n",
    "                    qdata = data[qt][qsub]\n",
    "            \n",
    "                    for i in tqdm(range(0, len(qdata), BATCH_SIZE), desc=f\"{qt} | {qsub}\"):\n",
    "                        batch_raw_prompts = qdata[i:i + BATCH_SIZE]\n",
    "            \n",
    "                        # Format prompts for vLLM (chat-style)\n",
    "                        batch_prompts = [\n",
    "                            apply_template(prompt, tokenizer, qt) for prompt in batch_raw_prompts\n",
    "                        ]\n",
    "            \n",
    "                        # Generate using vLLM\n",
    "                        request_output = llm.generate(\n",
    "                            prompts=batch_prompts,\n",
    "                            sampling_params=sampling_params,\n",
    "                            use_tqdm=False,\n",
    "                        )\n",
    "            \n",
    "                        # Store results: handle multiple outputs per prompt\n",
    "                        for j, prompt_dict in enumerate(batch_raw_prompts):\n",
    "                            if qt not in updated_rows:\n",
    "                                updated_rows[qt] = {}\n",
    "                            if qsub not in updated_rows[qt]:\n",
    "                                updated_rows[qt][qsub] = []\n",
    "                        \n",
    "                            generations = [out.text.strip() for out in request_output[j].outputs]\n",
    "                        \n",
    "                            # Clone original prompt and attach generations\n",
    "                            prompt_with_gen = prompt_dict.copy()\n",
    "                            prompt_with_gen[\"generations\"] = generations\n",
    "                        \n",
    "                            # Append to results list\n",
    "                            updated_rows[qt][qsub].append(prompt_with_gen)\n",
    "                            all_gens.extend(generations)\n",
    "            \n",
    "                        # Inside the batch loop:\n",
    "                        batch_filename = os.path.join(res_dir, f\"backup_Fact{fact_num}_{temp}.json\")\n",
    "                        with open(batch_filename, \"w\") as f:\n",
    "                            json.dump(updated_rows, f, indent=2)\n",
    "                        \n",
    "                        # print(f\"✅ BATCH {i + BATCH_SIZE} / {len(qdata)} DONE for {qt} | {qsub}\")\n",
    "            \n",
    "            # Final dump after all batches\n",
    "            final_filename = os.path.join(res_dir, f\"final_output_Fact{fact_num}_{temp}.json\")\n",
    "            with open(final_filename, \"w\") as f:\n",
    "                json.dump(updated_rows, f, indent=2)\n",
    "            \n",
    "            print(f\"✅ Fact {fact_num} Done\")\n",
    "            \n",
    "        print(f\"TEMPERATURE {temp} Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db397c5-5d72-4d5f-bbc4-fb88b9bb079e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
